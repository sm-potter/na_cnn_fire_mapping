{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ebfbe94-62bc-4552-8eaa-8022ead632f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Setup environment variables\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from MightyMosaic import MightyMosaic\n",
    "import segmentation_models as sm\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "import warnings\n",
    "import glob\n",
    "import tensorflow\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "086bca43-7dbf-4f1f-9871-1236db00164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image gen class to be used when predicting\n",
    "min_max_vi = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop = True)\n",
    "min_max_vi = min_max_vi[['6', '7', '8']]\n",
    "\n",
    "class img_gen_vi(tensorflow.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\n",
    "    Inputs are batch size, the image size, the input paths (x) and target paths (y)\n",
    "    \"\"\"\n",
    "\n",
    "    #will need pre defined variables batch_size, img_size, input_img_paths and target_img_paths\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "\t    self.batch_size = batch_size\n",
    "\t    self.img_size = img_size\n",
    "\t    self.input_img_paths = input_img_paths\n",
    "\t    self.target_img_paths = input_img_paths\n",
    "\n",
    "    #number of batches the generator is supposed to produceis the length of the paths divided by the batch siize\n",
    "    def __len__(self):\n",
    "\t    return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.input_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (x)\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (y)\n",
    "\t\t\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\") #create matrix of zeros which will have the dimension height, wideth, n_bands), 8 is the n_bands\n",
    "        \n",
    "  \n",
    "         #start populating x by enumerating over the input img paths\n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "\n",
    "           #load image\n",
    "            img =  np.round(np.load(path), 3)\n",
    "            \n",
    "            if img.shape[2] == 4:\n",
    "                \n",
    "                img = img[:, :, :-1]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                img = img[:, :, 6:9]\n",
    "\n",
    "            # img = img * 1000\n",
    "            img = img.astype(float)\n",
    "            img = np.round(img, 3)\n",
    "            img[img == 0] = -999\n",
    "\n",
    "            img[np.isnan(img)] = -999\n",
    "\n",
    "\n",
    "            img[img == -999] = np.nan\n",
    "\n",
    "            in_shape = img.shape\n",
    "            \n",
    "            #turn to dataframe to normalize\n",
    "            img = img.reshape(img.shape[0] * img.shape[1], img.shape[2])\n",
    "\t\t\t\n",
    "            img = pd.DataFrame(img)\n",
    "\t\t\t\n",
    "            img.columns = min_max_vi.columns\n",
    "\t\t\t\n",
    "            img = pd.concat([min_max_vi, img]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "            #normalize 0 to 1\n",
    "            img = pd.DataFrame(scaler.fit_transform(img))\n",
    "\t\t\t\n",
    "            img = img.iloc[2:]\n",
    "#\n",
    "#             img = img.values.reshape(in_shape)\n",
    "            img = img.values.reshape(in_shape)\n",
    "\n",
    "#             replace nan with -1\n",
    "            img[np.isnan(img)] = -1\n",
    "\n",
    "#apply standardization\n",
    "# img = normalize(img, axis=(0,1))\n",
    "\n",
    "            img = np.round(img, 3)\n",
    "            #populate x\n",
    "            x[j] = img#[:, :, 4:] index number is not included, \n",
    "\n",
    "\n",
    "        #do tthe same thing for y\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "\n",
    "            #load image\n",
    "            img =  np.round(np.load(path), 3)[:, :, -1]\n",
    "\n",
    "            img = img.astype(int)\n",
    "\n",
    "            img[img < 0] = 0\n",
    "            img[img >1] = 0\n",
    "            img[~np.isin(img, [0,1])] = 0\n",
    "\n",
    "            img[np.isnan(img)] = 0\n",
    "            img = img.astype(int)\n",
    "\n",
    "            # img =  tf.keras.utils.to_categorical(img, num_classes = 2)\n",
    "            # y[j] = np.expand_dims(img, 2) \n",
    "            y[j] = img\n",
    "  \n",
    "       \n",
    "    #Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "    # y[j] -= 1\n",
    "\n",
    "        return x, y\n",
    "\n",
    "def predict_model(model, generator, name):\n",
    "    \n",
    "    '''\n",
    "    model: tensorflow model to predict\n",
    "    generator: keras generator with the images to predict on\n",
    "    name: string, model name\\\n",
    "    fid: variable I was looping through\n",
    "    count: count retained earlier\n",
    "    '''\n",
    "    #get the results from the nbac and mtbs model\n",
    "    model_1_res = model.evaluate_generator(generator, 100)\n",
    "    # model_1_res = model.evaluate(models_vi_gen, \n",
    "    #                          steps=20,   # Total number of steps (batches)\n",
    "    #                          workers=4,                  # Number of workers for parallel data loading\n",
    "    #                          use_multiprocessing=True)   # Enable multiprocessing for faster data loading\n",
    "\n",
    "    iou = np.round(model_1_res[-2], 2)\n",
    "    precision = np.round(model_1_res[-5], 2)\n",
    "    recall = np.round(model_1_res[-4], 2)\n",
    "    f1 = np.round(model_1_res[-3], 2)\n",
    "    accuracy = np.round(model_1_res[-1], 2)\n",
    "\n",
    "    #make new dataframe with scores\n",
    "    in_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU': [iou],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "                        }, index=[0])  # Explicitly setting index to [0] for a single row\n",
    "\n",
    "    return in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdecc8-36a0-4eec-9d16-8ec80fa855e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 11:39:10.914008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Restrict TensorFlow to only use the second GPU (index 1)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set only GPU 1 as visible\n",
    "        tf.config.set_visible_devices(gpus[1], 'GPU')\n",
    "        # Optionally set memory growth if required\n",
    "        tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "        print(f\"Using GPU: {gpus[1]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Function to load models for a specific fold\n",
    "def load_models_for_fold(fold):\n",
    "    model_1 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_{fold}_old.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_2 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_3 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_sliding_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    return model_1, model_2, model_3\n",
    "\n",
    "# Filter function for chunked data\n",
    "def filter_chunked(in_names, chunked, data_type):\n",
    "    filtered_chunked = [name for name in chunked if int(name.split('_')[-1].split('.')[0]) in in_names]\n",
    "    base_path = f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_{data_type}_subs_0_128/\"\n",
    "    return [os.path.join(base_path, i) for i in filtered_chunked]\n",
    "\n",
    "# Function to predict using model and accumulate IoU across batches\n",
    "def predict_model(model, generator, name):\n",
    "    total_intersection = 0\n",
    "    total_union = 0\n",
    "    \n",
    "    for i in range(len(generator)):\n",
    "        x_batch, y_true = generator[i]\n",
    "        for j in range(len(x_batch)):\n",
    "            x_sample = np.expand_dims(x_batch[j], axis=0)\n",
    "            y_true_sample = y_true[j]\n",
    "\n",
    "            if np.all(y_true_sample == 0):\n",
    "                continue\n",
    "            \n",
    "            y_pred_sample = model.predict(x_sample, verbose=0)\n",
    "            y_pred_sample = np.squeeze(y_pred_sample, axis=1)[0]\n",
    "            y_pred_sample = np.where(y_pred_sample > 0.5, 1, 0)\n",
    "            y_pred_sample = y_pred_sample[:, :, 0]\n",
    "            \n",
    "            assert y_pred_sample.shape == y_true_sample.shape, f\"Shape mismatch: y_pred {y_pred_sample.shape} and y_true {y_true_sample.shape}\"\n",
    "            \n",
    "            intersection = np.logical_and(y_pred_sample, y_true_sample).sum()\n",
    "            union = np.logical_or(y_pred_sample, y_true_sample).sum()\n",
    "            \n",
    "            total_intersection += intersection\n",
    "            total_union += union\n",
    "    \n",
    "    iou_calculated = total_intersection / total_union if total_union > 0 else 0\n",
    "    \n",
    "    # Evaluate the model to get metrics including IOU (from model's perspective)\n",
    "    model_1_res = model.evaluate(generator, verbose=0)\n",
    "    \n",
    "    iou_model = np.round(model_1_res[-2], 2)\n",
    "    precision = np.round(model_1_res[-5], 2)\n",
    "    recall = np.round(model_1_res[-4], 2)\n",
    "    f1 = np.round(model_1_res[-3], 2)\n",
    "    accuracy = np.round(model_1_res[-1], 2)\n",
    "    \n",
    "    # Create a dataframe with the results\n",
    "    in_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU (Model)': [iou_model],\n",
    "        'IOU (Calculated)': [iou_calculated],\n",
    "        'Total Intersection': [total_intersection],\n",
    "        'Total Union': [total_union],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "    }, index=[0])\n",
    "    \n",
    "    return in_df\n",
    "\n",
    "# Function to process all folds dynamically for each model\n",
    "def process_all_folds(folds, batch_size, img_size, output_path):\n",
    "    total_intersections = {'old': 0, 'ndsi': 0, 'sliding': 0}\n",
    "    total_unions = {'old': 0, 'ndsi': 0, 'sliding': 0}\n",
    "    results = []\n",
    "\n",
    "    for fold in folds:\n",
    "        # Load models for the current fold\n",
    "        model_1, model_2, model_3 = load_models_for_fold(fold)\n",
    "        \n",
    "        # Load testing data for the fold\n",
    "        testing_names = pd.read_csv(f'/explore/nobackup/people/spotter5/cnn_mapping/Russia/test_fold_{fold}.csv')['ID'].tolist()\n",
    "\n",
    "        # Load chunked data for old, ndsi, and sliding\n",
    "        chunked_old = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_old_subs_0_128')\n",
    "        chunked_ndsi = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128')\n",
    "        chunked_sliding = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_sliding_subs_0_128')\n",
    "\n",
    "        # Filter chunked data based on test names\n",
    "        testing_names_old = filter_chunked(testing_names, chunked_old, 'old')\n",
    "        testing_names_ndsi = filter_chunked(testing_names, chunked_ndsi, 'monthly_ndsi')\n",
    "        testing_names_sliding = filter_chunked(testing_names, chunked_sliding, 'monthly_ndsi_sliding')\n",
    "\n",
    "        # Generate data for each model\n",
    "        model_vi_gen_old = img_gen_vi(batch_size, img_size, testing_names_old)\n",
    "        model_vi_gen_ndsi = img_gen_vi(batch_size, img_size, testing_names_ndsi)\n",
    "        model_vi_gen_sliding = img_gen_vi(batch_size, img_size, testing_names_sliding)\n",
    "\n",
    "        # Apply the generator and predict for each model\n",
    "        result_old = predict_model(model_1, model_vi_gen_old, f'Comb_Old_{fold}')\n",
    "        result_ndsi = predict_model(model_2, model_vi_gen_ndsi, f'Comb_NDSI_{fold}')\n",
    "        result_sliding = predict_model(model_3, model_vi_gen_sliding, f'Comb_Sliding_{fold}')\n",
    "\n",
    "        results.append(result_old)\n",
    "        results.append(result_ndsi)\n",
    "        results.append(result_sliding)\n",
    "\n",
    "        # Accumulate the intersections and unions\n",
    "        total_intersections['old'] += result_old['Total Intersection'].sum()\n",
    "        total_unions['old'] += result_old['Total Union'].sum()\n",
    "        total_intersections['ndsi'] += result_ndsi['Total Intersection'].sum()\n",
    "        total_unions['ndsi'] += result_ndsi['Total Union'].sum()\n",
    "        total_intersections['sliding'] += result_sliding['Total Intersection'].sum()\n",
    "        total_unions['sliding'] += result_sliding['Total Union'].sum()\n",
    "\n",
    "        # Save IOU for each fold\n",
    "        fold_iou = pd.concat([result_old, result_ndsi, result_sliding])\n",
    "        fold_output_file = os.path.join(output_path, f'combined_{fold}_iou.csv')\n",
    "        fold_iou.to_csv(fold_output_file, index=False)\n",
    "\n",
    "    # Calculate the final IoU for each model\n",
    "    iou_old_final = total_intersections['old'] / total_unions['old'] if total_unions['old'] != 0 else 0\n",
    "    iou_ndsi_final = total_intersections['ndsi'] / total_unions['ndsi'] if total_unions['ndsi'] != 0 else 0\n",
    "    iou_sliding_final = total_intersections['sliding'] / total_unions['sliding'] if total_unions['sliding'] != 0 else 0\n",
    "\n",
    "    # Create a final results dataframe\n",
    "    final_results = pd.DataFrame({\n",
    "        'Model': ['Overall_Old', 'Overall_NDSI', 'Overall_Sliding'],\n",
    "        'IOU (Calculated)': [iou_old_final, iou_ndsi_final, iou_sliding_final],\n",
    "        'Total Intersection': [total_intersections['old'], total_intersections['ndsi'], total_intersections['sliding']],\n",
    "        'Total Union': [total_unions['old'], total_unions['ndsi'], total_unions['sliding']]\n",
    "    })\n",
    "\n",
    "    # Concatenate fold results with overall results\n",
    "    all_results = pd.concat([pd.concat(results, ignore_index=True), final_results], ignore_index=True)\n",
    "\n",
    "    # Save results to CSV\n",
    "    output_file = os.path.join(output_path, 'combined_all_fold_iou_combined.csv')\n",
    "    all_results.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "    # Return the final results\n",
    "    return iou_old_final, iou_ndsi_final, iou_sliding_final\n",
    "\n",
    "# Main entry point\n",
    "# folds = [0, 2, 4]  # List of folds\n",
    "folds = [0, 1, 2]  # List of folds\n",
    "\n",
    "batch_size = 20  # Example batch size\n",
    "img_size = (128, 128)  # Example image size\n",
    "output_path = '/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Process all folds and get the final IoU for each model\n",
    "start_time = time.time()\n",
    "iou_old, iou_ndsi, iou_sliding = process_all_folds(folds, batch_size, img_size, output_path)\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = (end_time - start_time) / 60\n",
    "print(f\"Total execution time: {total_time:.2f} minutes\")\n",
    "\n",
    "# Print the final IoU for each model across all folds\n",
    "print(f\"Overall IoU for old model across all folds: {iou_old}\")\n",
    "print(f\"Overall IoU for NDSI model across all folds: {iou_ndsi}\")\n",
    "print(f\"Overall IoU for Sliding NDSI model across all folds: {iou_sliding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedce75-e83b-44f2-9f8a-a851f21a5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113ff31b-a2c8-4649-b036-632572cd8f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs available: \", gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf538f83-de2f-4b47-9384-07e34860a40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning3]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
