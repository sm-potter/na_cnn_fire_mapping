{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e826685-8263-4307-9b82-3616e9110dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Setup environment variables\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from MightyMosaic import MightyMosaic\n",
    "import segmentation_models as sm\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "import warnings\n",
    "import glob\n",
    "import tensorflow\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79233be-092f-45b0-9216-99f273a1562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image gen class to be used when predicting\n",
    "min_max_vi = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop = True)\n",
    "min_max_vi = min_max_vi[['6', '7', '8']]\n",
    "\n",
    "class img_gen_vi(tensorflow.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\n",
    "    Inputs are batch size, the image size, the input paths (x) and target paths (y)\n",
    "    \"\"\"\n",
    "\n",
    "    #will need pre defined variables batch_size, img_size, input_img_paths and target_img_paths\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "\t    self.batch_size = batch_size\n",
    "\t    self.img_size = img_size\n",
    "\t    self.input_img_paths = input_img_paths\n",
    "\t    self.target_img_paths = input_img_paths\n",
    "\n",
    "    #number of batches the generator is supposed to produceis the length of the paths divided by the batch siize\n",
    "    def __len__(self):\n",
    "\t    return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.input_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (x)\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (y)\n",
    "\t\t\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\") #create matrix of zeros which will have the dimension height, wideth, n_bands), 8 is the n_bands\n",
    "        \n",
    "  \n",
    "         #start populating x by enumerating over the input img paths\n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "\n",
    "           #load image\n",
    "            img =  np.round(np.load(path), 3)\n",
    "            \n",
    "            if img.shape[2] == 4:\n",
    "                \n",
    "                img = img[:, :, :-1]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                img = img[:, :, 6:9]\n",
    "\n",
    "            # img = img * 1000\n",
    "            img = img.astype(float)\n",
    "            img = np.round(img, 3)\n",
    "            img[img == 0] = -999\n",
    "\n",
    "            img[np.isnan(img)] = -999\n",
    "\n",
    "\n",
    "            img[img == -999] = np.nan\n",
    "\n",
    "            in_shape = img.shape\n",
    "            \n",
    "            #turn to dataframe to normalize\n",
    "            img = img.reshape(img.shape[0] * img.shape[1], img.shape[2])\n",
    "\t\t\t\n",
    "            img = pd.DataFrame(img)\n",
    "\t\t\t\n",
    "            img.columns = min_max_vi.columns\n",
    "\t\t\t\n",
    "            img = pd.concat([min_max_vi, img]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "            #normalize 0 to 1\n",
    "            img = pd.DataFrame(scaler.fit_transform(img))\n",
    "\t\t\t\n",
    "            img = img.iloc[2:]\n",
    "#\n",
    "#             img = img.values.reshape(in_shape)\n",
    "            img = img.values.reshape(in_shape)\n",
    "\n",
    "#             replace nan with -1\n",
    "            img[np.isnan(img)] = -1\n",
    "\n",
    "#apply standardization\n",
    "# img = normalize(img, axis=(0,1))\n",
    "\n",
    "            img = np.round(img, 3)\n",
    "            #populate x\n",
    "            x[j] = img#[:, :, 4:] index number is not included, \n",
    "\n",
    "\n",
    "        #do tthe same thing for y\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "\n",
    "            #load image\n",
    "            img =  np.round(np.load(path), 3)[:, :, -1]\n",
    "\n",
    "            img = img.astype(int)\n",
    "\n",
    "            img[img < 0] = 0\n",
    "            img[img >1] = 0\n",
    "            img[~np.isin(img, [0,1])] = 0\n",
    "\n",
    "            img[np.isnan(img)] = 0\n",
    "            img = img.astype(int)\n",
    "\n",
    "            # img =  tf.keras.utils.to_categorical(img, num_classes = 2)\n",
    "            # y[j] = np.expand_dims(img, 2) \n",
    "            y[j] = img\n",
    "  \n",
    "       \n",
    "    #Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "    # y[j] -= 1\n",
    "\n",
    "        return x, y\n",
    "\n",
    "def predict_model(model, generator, name):\n",
    "    \n",
    "    '''\n",
    "    model: tensorflow model to predict\n",
    "    generator: keras generator with the images to predict on\n",
    "    name: string, model name\\\n",
    "    fid: variable I was looping through\n",
    "    count: count retained earlier\n",
    "    '''\n",
    "    #get the results from the nbac and mtbs model\n",
    "    model_1_res = model.evaluate_generator(generator, 100)\n",
    "    # model_1_res = model.evaluate(models_vi_gen, \n",
    "    #                          steps=20,   # Total number of steps (batches)\n",
    "    #                          workers=4,                  # Number of workers for parallel data loading\n",
    "    #                          use_multiprocessing=True)   # Enable multiprocessing for faster data loading\n",
    "\n",
    "    iou = np.round(model_1_res[-2], 2)\n",
    "    precision = np.round(model_1_res[-5], 2)\n",
    "    recall = np.round(model_1_res[-4], 2)\n",
    "    f1 = np.round(model_1_res[-3], 2)\n",
    "    accuracy = np.round(model_1_res[-1], 2)\n",
    "\n",
    "    #make new dataframe with scores\n",
    "    in_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU': [iou],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "                        }, index=[0])  # Explicitly setting index to [0] for a single row\n",
    "\n",
    "    return in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204b7c89-32c1-4208-95f7-c4ff7b172f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Restrict TensorFlow to only use the second GPU (index 1)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set only GPU 1 as visible\n",
    "        tf.config.set_visible_devices(gpus[2], 'GPU')\n",
    "        # Optionally set memory growth if required\n",
    "        tf.config.experimental.set_memory_growth(gpus[2], True)\n",
    "        print(f\"Using GPU: {gpus[2]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0229fadc-e29d-4c42-9817-16a2745f8d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ecoregion Montane Sub-Arctic...\n",
      "Processing fold 0 for ecoregion Montane Sub-Arctic...\n",
      "Processing ecoregion Arctic Deserts and Tundra...\n",
      "Processing fold 0 for ecoregion Arctic Deserts and Tundra...\n",
      "Processing ecoregion Wetlands...\n",
      "Processing fold 0 for ecoregion Wetlands...\n",
      "Processing ecoregion Montane Boreal...\n",
      "Processing fold 0 for ecoregion Montane Boreal...\n",
      "Processing ecoregion Central Taiga...\n",
      "Processing fold 0 for ecoregion Central Taiga...\n",
      "Processing ecoregion Northern Taiga...\n",
      "Processing fold 0 for ecoregion Northern Taiga...\n",
      "Processing ecoregion Montane Sub-Boreal...\n",
      "Processing fold 0 for ecoregion Montane Sub-Boreal...\n",
      "Processing ecoregion Forest Tundra...\n",
      "Processing fold 0 for ecoregion Forest Tundra...\n",
      "Processing ecoregion Southern Taiga...\n",
      "Processing fold 0 for ecoregion Southern Taiga...\n",
      "Processing ecoregion Forest Steppe...\n",
      "Processing fold 0 for ecoregion Forest Steppe...\n",
      "                   ecoregion  final_iou_old  final_iou_ndsi  final_iou_sliding\n",
      "0         Montane Sub-Arctic       0.795615        0.771291           0.776177\n",
      "1  Arctic Deserts and Tundra       0.742433        0.669900           0.709562\n",
      "2                   Wetlands       0.820838        0.740119           0.778620\n",
      "3             Montane Boreal       0.776703        0.697554           0.726845\n",
      "4              Central Taiga       0.846147        0.755067           0.796217\n",
      "5             Northern Taiga       0.855040        0.809634           0.830274\n",
      "6         Montane Sub-Boreal       0.613589        0.481129           0.519507\n",
      "7              Forest Tundra       0.747689        0.703354           0.725135\n",
      "8             Southern Taiga       0.673107        0.502998           0.581192\n",
      "9              Forest Steppe       0.567554        0.001900           0.000000\n",
      "Total execution time: 147.09 minutes\n"
     ]
    }
   ],
   "source": [
    "# Function to load models for a specific fold\n",
    "def load_models_for_fold(fold):\n",
    "    model_1 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_{fold}_old.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_2 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_3 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_sliding_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_1 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_{fold}_old.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_2 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_ndsi_{fold}.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_3 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_ndsi_sliding_{fold}.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    return model_1, model_2, model_3\n",
    "\n",
    "# Function to filter chunked data for specific ecoregions\n",
    "def filter_chunked(in_names, chunked, data_type):\n",
    "    filtered_chunked = [name for name in chunked if int(name.split('_')[-1].split('.')[0]) in in_names]\n",
    "    base_path = f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_{data_type}_subs_0_128/\"\n",
    "    return [os.path.join(base_path, i) for i in filtered_chunked]\n",
    "\n",
    "# Function to predict using model and accumulate IoU using generator\n",
    "def predict_model_with_generator(model, generator, name):\n",
    "    total_intersection = 0\n",
    "    total_union = 0\n",
    "    \n",
    "    for i in range(len(generator)):\n",
    "        x_batch, y_true = generator[i]\n",
    "        for j in range(len(x_batch)):\n",
    "            x_sample = np.expand_dims(x_batch[j], axis=0)\n",
    "            y_true_sample = y_true[j]\n",
    "\n",
    "            if np.all(y_true_sample == 0):\n",
    "                continue\n",
    "            \n",
    "            y_pred_sample = model.predict(x_sample, verbose=0)\n",
    "            y_pred_sample = np.squeeze(y_pred_sample, axis=1)[0]\n",
    "            y_pred_sample = np.where(y_pred_sample > 0.5, 1, 0)\n",
    "            y_pred_sample = y_pred_sample[:, :, 0]\n",
    "            \n",
    "            intersection = np.logical_and(y_pred_sample, y_true_sample).sum()\n",
    "            union = np.logical_or(y_pred_sample, y_true_sample).sum()\n",
    "            \n",
    "            total_intersection += intersection\n",
    "            total_union += union\n",
    "\n",
    "    iou_calculated = total_intersection / total_union if total_union > 0 else 0\n",
    "    \n",
    "    # Evaluate the model to get metrics including IOU (from model's perspective)\n",
    "    model_1_res = model.evaluate(generator, verbose=0)\n",
    "    \n",
    "    iou_model = np.round(model_1_res[-2], 2)\n",
    "    precision = np.round(model_1_res[-5], 2)\n",
    "    recall = np.round(model_1_res[-4], 2)\n",
    "    f1 = np.round(model_1_res[-3], 2)\n",
    "    accuracy = np.round(model_1_res[-1], 2)\n",
    "    \n",
    "    # Create a dataframe with the results\n",
    "    in_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU (Model)': [iou_model],\n",
    "        'IOU (Calculated)': [iou_calculated],\n",
    "        'Total Intersection': [total_intersection],\n",
    "        'Total Union': [total_union],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "    }, index=[0])\n",
    "    \n",
    "    return in_df, iou_calculated\n",
    "\n",
    "# Main function to calculate IoU for all ecoregions and folds\n",
    "def calculate_iou_across_folds_ecoregion(ecoregions, eco_df, folds, batch_size, img_size):\n",
    "    iou_results = []\n",
    "    \n",
    "    for ecoregion in ecoregions:\n",
    "        print(f\"Processing ecoregion {ecoregion}...\")\n",
    "\n",
    "        total_intersections = {'old': 0, 'ndsi': 0, 'sliding': 0}\n",
    "        total_unions = {'old': 0, 'ndsi': 0, 'sliding': 0}\n",
    "        iou_per_fold = []\n",
    "\n",
    "        sub_eco = eco_df[eco_df['ecoregion'] == ecoregion]\n",
    "        \n",
    "        for fold in folds:\n",
    "            print(f\"Processing fold {fold} for ecoregion {ecoregion}...\")\n",
    "            \n",
    "            # Load models for the fold\n",
    "            model_1, model_2, model_3 = load_models_for_fold(fold)\n",
    "            \n",
    "            # Load testing data for the fold\n",
    "            testing_names = pd.read_csv(f'/explore/nobackup/people/spotter5/cnn_mapping/Russia/test_fold_{fold}.csv')['ID'].tolist()\n",
    "            \n",
    "            # Filter ecoregion and testing names\n",
    "            sub_fold = sub_eco[sub_eco['ID'].isin(testing_names)]\n",
    "            fold_ids = sub_fold['ID'].unique().tolist()\n",
    "            \n",
    "            if len(fold_ids) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Load chunked data\n",
    "            chunked_old = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_old_subs_0_128')\n",
    "            chunked_ndsi = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128')\n",
    "            chunked_sliding = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_sliding_subs_0_128')\n",
    "            \n",
    "            # Filter chunked data by fold IDs and include full paths\n",
    "            testing_names_old = filter_chunked(fold_ids, chunked_old, 'old')\n",
    "            testing_names_ndsi = filter_chunked(fold_ids, chunked_ndsi, 'monthly_ndsi')\n",
    "            testing_names_sliding = filter_chunked(fold_ids, chunked_sliding, 'monthly_ndsi_sliding')\n",
    "\n",
    "            # Generate data for each model\n",
    "            model_vi_gen_old = img_gen_vi(batch_size, img_size, testing_names_old)\n",
    "            model_vi_gen_ndsi = img_gen_vi(batch_size, img_size, testing_names_ndsi)\n",
    "            model_vi_gen_sliding = img_gen_vi(batch_size, img_size, testing_names_sliding)\n",
    "\n",
    "            # Apply the generator and predict for each model\n",
    "            result_old, iou_old = predict_model_with_generator(model_1, model_vi_gen_old, f'Comb_Old_{fold}')\n",
    "            result_ndsi, iou_ndsi = predict_model_with_generator(model_2, model_vi_gen_ndsi, f'Comb_NDSI_{fold}')\n",
    "            result_sliding, iou_sliding = predict_model_with_generator(model_3, model_vi_gen_sliding, f'Comb_Sliding_{fold}')\n",
    "            \n",
    "            # Record IoU for this fold\n",
    "            iou_per_fold.append({\n",
    "                'ecoregion': ecoregion,\n",
    "                'fold': fold,\n",
    "                'iou_old': iou_old,\n",
    "                'iou_ndsi': iou_ndsi,\n",
    "                'iou_sliding': iou_sliding\n",
    "            })\n",
    "\n",
    "            # Accumulate the intersections and unions\n",
    "            total_intersections['old'] += result_old['Total Intersection'].sum()\n",
    "            total_unions['old'] += result_old['Total Union'].sum()\n",
    "            total_intersections['ndsi'] += result_ndsi['Total Intersection'].sum()\n",
    "            total_unions['ndsi'] += result_ndsi['Total Union'].sum()\n",
    "            total_intersections['sliding'] += result_sliding['Total Intersection'].sum()\n",
    "            total_unions['sliding'] += result_sliding['Total Union'].sum()\n",
    "\n",
    "        # Calculate final IoU for this ecoregion across all folds using the sum of intersections and unions\n",
    "        iou_old_final = total_intersections['old'] / total_unions['old'] if total_unions['old'] != 0 else 0\n",
    "        iou_ndsi_final = total_intersections['ndsi'] / total_unions['ndsi'] if total_unions['ndsi'] != 0 else 0\n",
    "        iou_sliding_final = total_intersections['sliding'] / total_unions['sliding'] if total_unions['sliding'] != 0 else 0\n",
    "        \n",
    "        # Store final results\n",
    "        iou_results.append({\n",
    "            'ecoregion': ecoregion,\n",
    "            'final_iou_old': iou_old_final,\n",
    "            'final_iou_ndsi': iou_ndsi_final,\n",
    "            'final_iou_sliding': iou_sliding_final\n",
    "        })\n",
    "\n",
    "    # Return fold-specific and final IoU results\n",
    "    return pd.DataFrame(iou_per_fold), pd.DataFrame(iou_results)\n",
    "\n",
    "# List of folds to process\n",
    "# folds = [0, 2, 4]\n",
    "\n",
    "folds= [0]\n",
    "\n",
    "# Load ecoregion shapefile\n",
    "eco = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_eco_clip.shp')\n",
    "all_ecoregions = eco['ecoregion'].unique().tolist()\n",
    "\n",
    "# Parameters for image generator\n",
    "batch_size = 20\n",
    "img_size = (128, 128)\n",
    "\n",
    "start_time = time.time()\n",
    "# Calculate IoU across all ecoregions and folds\n",
    "iou_per_fold_df, final_iou_df = calculate_iou_across_folds_ecoregion(all_ecoregions, eco, folds, batch_size, img_size)\n",
    "\n",
    "# Save the results to CSV\n",
    "# iou_per_fold_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_folds.csv', index=False)\n",
    "# final_iou_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_final.csv', index=False)\n",
    "\n",
    "iou_per_fold_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_folds_0.csv', index=False)\n",
    "final_iou_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_final_0.csv', index=False)\n",
    "\n",
    "# Print the final IoU values for each ecoregion\n",
    "print(final_iou_df)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = (end_time - start_time) / 60\n",
    "print(f\"Total execution time: {total_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "199042b8-41c9-4cfe-9a24-cd688de85016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a21d1b-d726-4471-935d-78be576f17f9",
   "metadata": {},
   "source": [
    "Use old method generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ffcbb6-4f75-4a2b-b88b-a798da7aef1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "Processing fold 0...\n",
      "Processing ecoregion Montane Sub-Arctic in fold 0...\n",
      "Processing ecoregion Arctic Deserts and Tundra in fold 0...\n",
      "Processing ecoregion Wetlands in fold 0...\n",
      "Processing ecoregion Montane Boreal in fold 0...\n",
      "Processing ecoregion Central Taiga in fold 0...\n",
      "Processing ecoregion Northern Taiga in fold 0...\n",
      "Processing ecoregion Montane Sub-Boreal in fold 0...\n",
      "Processing ecoregion Forest Tundra in fold 0...\n",
      "Processing ecoregion Southern Taiga in fold 0...\n",
      "Processing ecoregion Forest Steppe in fold 0...\n",
      "Results saved for fold 0.\n",
      "Processing fold 1...\n",
      "Processing ecoregion Montane Sub-Arctic in fold 1...\n",
      "Processing ecoregion Arctic Deserts and Tundra in fold 1...\n",
      "Processing ecoregion Wetlands in fold 1...\n",
      "Processing ecoregion Montane Boreal in fold 1...\n",
      "Processing ecoregion Central Taiga in fold 1...\n",
      "Processing ecoregion Northern Taiga in fold 1...\n",
      "Processing ecoregion Montane Sub-Boreal in fold 1...\n",
      "Processing ecoregion Forest Tundra in fold 1...\n",
      "Processing ecoregion Southern Taiga in fold 1...\n",
      "Processing ecoregion Forest Steppe in fold 1...\n",
      "Results saved for fold 1.\n",
      "Processing fold 2...\n",
      "Processing ecoregion Montane Sub-Arctic in fold 2...\n",
      "Processing ecoregion Arctic Deserts and Tundra in fold 2...\n",
      "Processing ecoregion Wetlands in fold 2...\n",
      "Processing ecoregion Montane Boreal in fold 2...\n",
      "Processing ecoregion Central Taiga in fold 2...\n",
      "Processing ecoregion Northern Taiga in fold 2...\n",
      "Processing ecoregion Montane Sub-Boreal in fold 2...\n",
      "Processing ecoregion Forest Tundra in fold 2...\n",
      "Processing ecoregion Southern Taiga in fold 2...\n",
      "Processing ecoregion Forest Steppe in fold 2...\n",
      "Results saved for fold 2.\n",
      "Processing fold 3...\n",
      "Processing ecoregion Montane Sub-Arctic in fold 3...\n",
      "Processing ecoregion Arctic Deserts and Tundra in fold 3...\n",
      "Processing ecoregion Wetlands in fold 3...\n",
      "Processing ecoregion Montane Boreal in fold 3...\n",
      "Processing ecoregion Central Taiga in fold 3...\n",
      "Processing ecoregion Northern Taiga in fold 3...\n",
      "Processing ecoregion Montane Sub-Boreal in fold 3...\n",
      "Processing ecoregion Forest Tundra in fold 3...\n",
      "Processing ecoregion Southern Taiga in fold 3...\n",
      "Processing ecoregion Forest Steppe in fold 3...\n",
      "Results saved for fold 3.\n",
      "Processing fold 4...\n",
      "Processing ecoregion Montane Sub-Arctic in fold 4...\n",
      "Processing ecoregion Arctic Deserts and Tundra in fold 4...\n",
      "Processing ecoregion Wetlands in fold 4...\n",
      "Processing ecoregion Montane Boreal in fold 4...\n",
      "Processing ecoregion Central Taiga in fold 4...\n",
      "Processing ecoregion Northern Taiga in fold 4...\n",
      "Processing ecoregion Montane Sub-Boreal in fold 4...\n",
      "Processing ecoregion Forest Tundra in fold 4...\n",
      "Processing ecoregion Southern Taiga in fold 4...\n",
      "Processing ecoregion Forest Steppe in fold 4...\n",
      "Results saved for fold 4.\n",
      "Total execution time: 157.61 minutes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import geopandas as gpd\n",
    "import segmentation_models as sm\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Image generator class for prediction\n",
    "min_max_vi = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop=True)\n",
    "min_max_vi = min_max_vi[['6', '7', '8']]\n",
    "\n",
    "class img_gen_vi(tensorflow.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = input_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.input_img_paths[i:i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i:i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        \n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "            img = np.round(np.load(path), 3)\n",
    "            if img.shape[2] == 4:\n",
    "                img = img[:, :, :-1]\n",
    "            else:\n",
    "                img = img[:, :, 6:9]\n",
    "            img = img.astype(float)\n",
    "            img[img == 0] = -999\n",
    "            img[np.isnan(img)] = -999\n",
    "            img[img == -999] = np.nan\n",
    "            in_shape = img.shape\n",
    "            img = pd.DataFrame(img.reshape(img.shape[0] * img.shape[1], img.shape[2]))\n",
    "            img.columns = min_max_vi.columns\n",
    "            img = pd.concat([min_max_vi, img]).reset_index(drop=True)\n",
    "            img = pd.DataFrame(scaler.fit_transform(img))\n",
    "            img = img.iloc[2:].values.reshape(in_shape)\n",
    "            img[np.isnan(img)] = -1\n",
    "            x[j] = img\n",
    "\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = np.round(np.load(path), 3)[:, :, -1]\n",
    "            img = img.astype(int)\n",
    "            img[img < 0] = 0\n",
    "            img[img > 1] = 0\n",
    "            img[np.isnan(img)] = 0\n",
    "            y[j] = img\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Prediction function with generator\n",
    "def predict_model(model, generator, name):\n",
    "    model_res = model.evaluate(generator, verbose=0)\n",
    "    iou = np.round(model_res[-2], 2)\n",
    "    precision = np.round(model_res[-5], 2)\n",
    "    recall = np.round(model_res[-4], 2)\n",
    "    f1 = np.round(model_res[-3], 2)\n",
    "    accuracy = np.round(model_res[-1], 2)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU': [iou],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "    })\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Restrict TensorFlow to only use the second GPU (index 1)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[1], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "        print(f\"Using GPU: {gpus[1]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Load models for a specific fold\n",
    "def load_models_for_fold(fold):\n",
    "    model_1 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_{fold}_old.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_2 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_3 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_sliding_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    return model_1, model_2, model_3\n",
    "\n",
    "# Filter chunked data based on IDs\n",
    "def filter_chunked(in_names, chunked, data_type):\n",
    "    filtered_chunked = [name for name in chunked if int(name.split('_')[-1].split('.')[0]) in in_names]\n",
    "    base_path = f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_{data_type}_subs_0_128/\"\n",
    "    return [os.path.join(base_path, i) for i in filtered_chunked]\n",
    "\n",
    "# Main function to process and save results per fold for each ecoregion\n",
    "def process_folds_ecoregions(folds, batch_size, img_size, output_path, eco_df, all_ecoregions):\n",
    "    for fold in folds:\n",
    "        print(f\"Processing fold {fold}...\")\n",
    "\n",
    "        # Load models for the current fold\n",
    "        model_1, model_2, model_3 = load_models_for_fold(fold)\n",
    "\n",
    "        fold_results = []\n",
    "\n",
    "                        \n",
    "        # Load testing data for the fold\n",
    "        testing_names = pd.read_csv(f'/explore/nobackup/people/spotter5/cnn_mapping/Russia/test_fold_{fold}.csv')['ID'].tolist()\n",
    "        \n",
    "        for ecoregion in all_ecoregions:\n",
    "            print(f\"Processing ecoregion {ecoregion} in fold {fold}...\")\n",
    "            \n",
    "            # Filter the data for the current ecoregion\n",
    "            # sub_eco = eco_df[eco_df['ecoregion'] == ecoregion]\n",
    "\n",
    "            # #filter in testing names\n",
    "            # sub_eco = sub_eco[sub_eco['ID'].isin(testing_names)]\n",
    "\n",
    "            \n",
    "            # testing_names = sub_eco['ID'].tolist()\n",
    "\n",
    "            # Filter the data for the current ecoregion and intersect with testing names\n",
    "            sub_eco = eco_df[(eco_df['ecoregion'] == ecoregion) & (eco_df['ID'].isin(testing_names))]\n",
    "            \n",
    "            if sub_eco.empty:\n",
    "                continue  # Skip if there are no matching IDs in this ecoregion and fold\n",
    "\n",
    "            testing_names_ecoregion = sub_eco['ID'].tolist()\n",
    "\n",
    "            # if len(testing_names) == 0:\n",
    "            #     continue\n",
    "\n",
    "            # Load chunked data for old, ndsi, and sliding\n",
    "            chunked_old = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_old_subs_0_128')\n",
    "            chunked_ndsi = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128')\n",
    "            chunked_sliding = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_sliding_subs_0_128')\n",
    "\n",
    "            # Filter chunked data based on test names for the current ecoregion\n",
    "            testing_names_old = filter_chunked(testing_names_ecoregion, chunked_old, 'old')\n",
    "            testing_names_ndsi = filter_chunked(testing_names_ecoregion, chunked_ndsi, 'monthly_ndsi')\n",
    "            testing_names_sliding = filter_chunked(testing_names_ecoregion, chunked_sliding, 'monthly_ndsi_sliding')\n",
    "\n",
    "            # Generate data for each model\n",
    "            model_vi_gen_old = img_gen_vi(batch_size, img_size, testing_names_old)\n",
    "            model_vi_gen_ndsi = img_gen_vi(batch_size, img_size, testing_names_ndsi)\n",
    "            model_vi_gen_sliding = img_gen_vi(batch_size, img_size, testing_names_sliding)\n",
    "\n",
    "            # Apply the generator and predict for each model\n",
    "            result_old = predict_model(model_1, model_vi_gen_old, f'Comb_Old_{fold}_{ecoregion}')\n",
    "            result_ndsi = predict_model(model_2, model_vi_gen_ndsi, f'Comb_NDSI_{fold}_{ecoregion}')\n",
    "            result_sliding = predict_model(model_3, model_vi_gen_sliding, f'Comb_Sliding_{fold}_{ecoregion}')\n",
    "\n",
    "            # Add the ecoregion information to the result\n",
    "            result_old['Ecoregion'] = ecoregion\n",
    "            result_ndsi['Ecoregion'] = ecoregion\n",
    "            result_sliding['Ecoregion'] = ecoregion\n",
    "\n",
    "            # Append the results to the fold results list\n",
    "            fold_results.append(pd.concat([result_old, result_ndsi, result_sliding], ignore_index=True))\n",
    "\n",
    "        # Combine all ecoregion results for the fold and save to CSV\n",
    "        fold_results_df = pd.concat(fold_results, ignore_index=True)\n",
    "        fold_results_df.to_csv(os.path.join(output_path, f'combined_ecoregions_{fold}.csv'), index=False)\n",
    "\n",
    "        print(f\"Results saved for fold {fold}.\")\n",
    "\n",
    "# Parameters and entry point\n",
    "# folds = [0, 1, 2, 4]  # Example fold numbers\n",
    "folds = range(0, 5)\n",
    "batch_size = 20  # Example batch size\n",
    "img_size = (128, 128)  # Example image size\n",
    "output_path = '/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Load ecoregion shapefile\n",
    "eco = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_eco_clip.shp')\n",
    "all_ecoregions = eco['ecoregion'].unique().tolist()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each fold and each ecoregion\n",
    "process_folds_ecoregions(folds, batch_size, img_size, output_path, eco, all_ecoregions)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = (end_time - start_time) / 60\n",
    "print(f\"Total execution time: {total_time:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b91dde-0db4-4d0f-8bab-3b8c97a53ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning3]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
