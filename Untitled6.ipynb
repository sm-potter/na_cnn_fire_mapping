{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd41b61-2fdb-453a-9de1-455f8f4d0a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Setup environment variables\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from MightyMosaic import MightyMosaic\n",
    "import segmentation_models as sm\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "import warnings\n",
    "import glob\n",
    "import tensorflow\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706665b8-2fbb-4068-9e6b-9ea69b51f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image gen class to be used when predicting\n",
    "min_max_vi = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop = True)\n",
    "min_max_vi = min_max_vi[['6', '7', '8']]\n",
    "\n",
    "class img_gen_vi(tensorflow.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\n",
    "    Inputs are batch size, the image size, the input paths (x) and target paths (y)\n",
    "    \"\"\"\n",
    "\n",
    "    #will need pre defined variables batch_size, img_size, input_img_paths and target_img_paths\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "\t    self.batch_size = batch_size\n",
    "\t    self.img_size = img_size\n",
    "\t    self.input_img_paths = input_img_paths\n",
    "\t    self.target_img_paths = input_img_paths\n",
    "\n",
    "    #number of batches the generator is supposed to produceis the length of the paths divided by the batch siize\n",
    "    def __len__(self):\n",
    "\t    return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.input_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (x)\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (y)\n",
    "\t\t\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\") #create matrix of zeros which will have the dimension height, wideth, n_bands), 8 is the n_bands\n",
    "        \n",
    "  \n",
    "         #start populating x by enumerating over the input img paths\n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "\n",
    "           #load image\n",
    "            img =  np.round(np.load(path), 3)\n",
    "            \n",
    "            if img.shape[2] == 4:\n",
    "                \n",
    "                img = img[:, :, :-1]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                img = img[:, :, 6:9]\n",
    "\n",
    "            # img = img * 1000\n",
    "            img = img.astype(float)\n",
    "            img = np.round(img, 3)\n",
    "            img[img == 0] = -999\n",
    "\n",
    "            img[np.isnan(img)] = -999\n",
    "\n",
    "\n",
    "            img[img == -999] = np.nan\n",
    "\n",
    "            in_shape = img.shape\n",
    "            \n",
    "            #turn to dataframe to normalize\n",
    "            img = img.reshape(img.shape[0] * img.shape[1], img.shape[2])\n",
    "\t\t\t\n",
    "            img = pd.DataFrame(img)\n",
    "\t\t\t\n",
    "            img.columns = min_max_vi.columns\n",
    "\t\t\t\n",
    "            img = pd.concat([min_max_vi, img]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "            #normalize 0 to 1\n",
    "            img = pd.DataFrame(scaler.fit_transform(img))\n",
    "\t\t\t\n",
    "            img = img.iloc[2:]\n",
    "#\n",
    "#             img = img.values.reshape(in_shape)\n",
    "            img = img.values.reshape(in_shape)\n",
    "\n",
    "#             replace nan with -1\n",
    "            img[np.isnan(img)] = -1\n",
    "\n",
    "#apply standardization\n",
    "# img = normalize(img, axis=(0,1))\n",
    "\n",
    "            img = np.round(img, 3)\n",
    "            #populate x\n",
    "            x[j] = img#[:, :, 4:] index number is not included, \n",
    "\n",
    "\n",
    "        #do tthe same thing for y\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "\n",
    "            #load image\n",
    "            img =  np.round(np.load(path), 3)[:, :, -1]\n",
    "\n",
    "            img = img.astype(int)\n",
    "\n",
    "            img[img < 0] = 0\n",
    "            img[img >1] = 0\n",
    "            img[~np.isin(img, [0,1])] = 0\n",
    "\n",
    "            img[np.isnan(img)] = 0\n",
    "            img = img.astype(int)\n",
    "\n",
    "            # img =  tf.keras.utils.to_categorical(img, num_classes = 2)\n",
    "            # y[j] = np.expand_dims(img, 2) \n",
    "            y[j] = img\n",
    "  \n",
    "       \n",
    "    #Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "    # y[j] -= 1\n",
    "\n",
    "        return x, y\n",
    "\n",
    "def predict_model(model, generator, name):\n",
    "    \n",
    "    '''\n",
    "    model: tensorflow model to predict\n",
    "    generator: keras generator with the images to predict on\n",
    "    name: string, model name\\\n",
    "    fid: variable I was looping through\n",
    "    count: count retained earlier\n",
    "    '''\n",
    "    #get the results from the nbac and mtbs model\n",
    "    model_1_res = model.evaluate_generator(generator, 100)\n",
    "    # model_1_res = model.evaluate(models_vi_gen, \n",
    "    #                          steps=20,   # Total number of steps (batches)\n",
    "    #                          workers=4,                  # Number of workers for parallel data loading\n",
    "    #                          use_multiprocessing=True)   # Enable multiprocessing for faster data loading\n",
    "\n",
    "    iou = np.round(model_1_res[-2], 2)\n",
    "    precision = np.round(model_1_res[-5], 2)\n",
    "    recall = np.round(model_1_res[-4], 2)\n",
    "    f1 = np.round(model_1_res[-3], 2)\n",
    "    accuracy = np.round(model_1_res[-1], 2)\n",
    "\n",
    "    #make new dataframe with scores\n",
    "    in_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU': [iou],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "                        }, index=[0])  # Explicitly setting index to [0] for a single row\n",
    "\n",
    "    return in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834eab4-8c52-4af5-beb7-b02ca497d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict TensorFlow to only use the second GPU (index 1)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set only GPU 1 as visible\n",
    "        tf.config.set_visible_devices(gpus[3], 'GPU')\n",
    "        # Optionally set memory growth if required\n",
    "        tf.config.experimental.set_memory_growth(gpus[3], True)\n",
    "        print(f\"Using GPU: {gpus[3]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94378e6-f618-410a-912f-f9015bd8a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load models for a specific fold\n",
    "def load_models_for_fold(fold):\n",
    "    model_1 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_{fold}_old.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_2 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_3 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_sliding_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_1 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_{fold}_old.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_2 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_ndsi_{fold}.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_3 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_ndsi_sliding_{fold}.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    return model_1, model_2, model_3\n",
    "\n",
    "# Function to filter chunked data for specific ecoregions\n",
    "def filter_chunked(in_names, chunked, data_type):\n",
    "    filtered_chunked = [name for name in chunked if int(name.split('_')[-1].split('.')[0]) in in_names]\n",
    "    base_path = f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_{data_type}_subs_0_128/\"\n",
    "    return [os.path.join(base_path, i) for i in filtered_chunked]\n",
    "\n",
    "# Function to predict using model and accumulate IoU using generator\n",
    "def predict_model_with_generator(model, generator, name):\n",
    "    total_intersection = 0\n",
    "    total_union = 0\n",
    "    \n",
    "    for i in range(len(generator)):\n",
    "        x_batch, y_true = generator[i]\n",
    "        for j in range(len(x_batch)):\n",
    "            x_sample = np.expand_dims(x_batch[j], axis=0)\n",
    "            y_true_sample = y_true[j]\n",
    "\n",
    "            if np.all(y_true_sample == 0):\n",
    "                continue\n",
    "            \n",
    "            y_pred_sample = model.predict(x_sample, verbose=0)\n",
    "            y_pred_sample = np.squeeze(y_pred_sample, axis=1)[0]\n",
    "            y_pred_sample = np.where(y_pred_sample > 0.5, 1, 0)\n",
    "            y_pred_sample = y_pred_sample[:, :, 0]\n",
    "            \n",
    "            intersection = np.logical_and(y_pred_sample, y_true_sample).sum()\n",
    "            union = np.logical_or(y_pred_sample, y_true_sample).sum()\n",
    "            \n",
    "            total_intersection += intersection\n",
    "            total_union += union\n",
    "\n",
    "    iou_calculated = total_intersection / total_union if total_union > 0 else 0\n",
    "    \n",
    "    # Evaluate the model to get metrics including IOU (from model's perspective)\n",
    "    model_1_res = model.evaluate(generator, verbose=0)\n",
    "    \n",
    "    iou_model = np.round(model_1_res[-2], 2)\n",
    "    precision = np.round(model_1_res[-5], 2)\n",
    "    recall = np.round(model_1_res[-4], 2)\n",
    "    f1 = np.round(model_1_res[-3], 2)\n",
    "    accuracy = np.round(model_1_res[-1], 2)\n",
    "    \n",
    "    # Create a dataframe with the results\n",
    "    in_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU (Model)': [iou_model],\n",
    "        'IOU (Calculated)': [iou_calculated],\n",
    "        'Total Intersection': [total_intersection],\n",
    "        'Total Union': [total_union],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "    }, index=[0])\n",
    "    \n",
    "    return in_df, iou_calculated\n",
    "\n",
    "# Main function to calculate IoU for all ecoregions and folds\n",
    "def calculate_iou_across_folds_ecoregion(ecoregions, eco_df, folds, batch_size, img_size):\n",
    "    iou_results = []\n",
    "    \n",
    "    for ecoregion in ecoregions:\n",
    "        print(f\"Processing ecoregion {ecoregion}...\")\n",
    "\n",
    "        total_intersections = {'old': 0, 'ndsi': 0, 'sliding': 0}\n",
    "        total_unions = {'old': 0, 'ndsi': 0, 'sliding': 0}\n",
    "        iou_per_fold = []\n",
    "\n",
    "        sub_eco = eco_df[eco_df['ecoregion'] == ecoregion]\n",
    "        \n",
    "        for fold in folds:\n",
    "            print(f\"Processing fold {fold} for ecoregion {ecoregion}...\")\n",
    "            \n",
    "            # Load models for the fold\n",
    "            model_1, model_2, model_3 = load_models_for_fold(fold)\n",
    "            \n",
    "            # Load testing data for the fold\n",
    "            testing_names = pd.read_csv(f'/explore/nobackup/people/spotter5/cnn_mapping/Russia/test_fold_{fold}.csv')['ID'].tolist()\n",
    "            \n",
    "            # Filter ecoregion and testing names\n",
    "            sub_fold = sub_eco[sub_eco['ID'].isin(testing_names)]\n",
    "            fold_ids = sub_fold['ID'].unique().tolist()\n",
    "            \n",
    "            if len(fold_ids) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Load chunked data\n",
    "            chunked_old = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_old_subs_0_128')\n",
    "            chunked_ndsi = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128')\n",
    "            chunked_sliding = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_sliding_subs_0_128')\n",
    "            \n",
    "            # Filter chunked data by fold IDs and include full paths\n",
    "            testing_names_old = filter_chunked(fold_ids, chunked_old, 'old')\n",
    "            testing_names_ndsi = filter_chunked(fold_ids, chunked_ndsi, 'monthly_ndsi')\n",
    "            testing_names_sliding = filter_chunked(fold_ids, chunked_sliding, 'monthly_ndsi_sliding')\n",
    "\n",
    "            # Generate data for each model\n",
    "            model_vi_gen_old = img_gen_vi(batch_size, img_size, testing_names_old)\n",
    "            model_vi_gen_ndsi = img_gen_vi(batch_size, img_size, testing_names_ndsi)\n",
    "            model_vi_gen_sliding = img_gen_vi(batch_size, img_size, testing_names_sliding)\n",
    "\n",
    "            # Apply the generator and predict for each model\n",
    "            result_old, iou_old = predict_model_with_generator(model_1, model_vi_gen_old, f'Comb_Old_{fold}')\n",
    "            result_ndsi, iou_ndsi = predict_model_with_generator(model_2, model_vi_gen_ndsi, f'Comb_NDSI_{fold}')\n",
    "            result_sliding, iou_sliding = predict_model_with_generator(model_3, model_vi_gen_sliding, f'Comb_Sliding_{fold}')\n",
    "            \n",
    "            # Record IoU for this fold\n",
    "            iou_per_fold.append({\n",
    "                'ecoregion': ecoregion,\n",
    "                'fold': fold,\n",
    "                'iou_old': iou_old,\n",
    "                'iou_ndsi': iou_ndsi,\n",
    "                'iou_sliding': iou_sliding\n",
    "            })\n",
    "\n",
    "            # Accumulate the intersections and unions\n",
    "            total_intersections['old'] += result_old['Total Intersection'].sum()\n",
    "            total_unions['old'] += result_old['Total Union'].sum()\n",
    "            total_intersections['ndsi'] += result_ndsi['Total Intersection'].sum()\n",
    "            total_unions['ndsi'] += result_ndsi['Total Union'].sum()\n",
    "            total_intersections['sliding'] += result_sliding['Total Intersection'].sum()\n",
    "            total_unions['sliding'] += result_sliding['Total Union'].sum()\n",
    "\n",
    "        # Calculate final IoU for this ecoregion across all folds using the sum of intersections and unions\n",
    "        iou_old_final = total_intersections['old'] / total_unions['old'] if total_unions['old'] != 0 else 0\n",
    "        iou_ndsi_final = total_intersections['ndsi'] / total_unions['ndsi'] if total_unions['ndsi'] != 0 else 0\n",
    "        iou_sliding_final = total_intersections['sliding'] / total_unions['sliding'] if total_unions['sliding'] != 0 else 0\n",
    "        \n",
    "        # Store final results\n",
    "        iou_results.append({\n",
    "            'ecoregion': ecoregion,\n",
    "            'final_iou_old': iou_old_final,\n",
    "            'final_iou_ndsi': iou_ndsi_final,\n",
    "            'final_iou_sliding': iou_sliding_final\n",
    "        })\n",
    "\n",
    "    # Return fold-specific and final IoU results\n",
    "    return pd.DataFrame(iou_per_fold), pd.DataFrame(iou_results)\n",
    "\n",
    "# List of folds to process\n",
    "# folds = [0, 2, 4]\n",
    "\n",
    "folds= [0]\n",
    "\n",
    "# Load ecoregion shapefile\n",
    "eco = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_eco_clip.shp')\n",
    "all_ecoregions = eco['ecoregion'].unique().tolist()\n",
    "\n",
    "# Parameters for image generator\n",
    "batch_size = 20\n",
    "img_size = (128, 128)\n",
    "\n",
    "start_time = time.time()\n",
    "# Calculate IoU across all ecoregions and folds\n",
    "iou_per_fold_df, final_iou_df = calculate_iou_across_folds_ecoregion(all_ecoregions, eco, folds, batch_size, img_size)\n",
    "\n",
    "# Save the results to CSV\n",
    "# iou_per_fold_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_folds.csv', index=False)\n",
    "# final_iou_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_final.csv', index=False)\n",
    "\n",
    "iou_per_fold_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_folds_0.csv', index=False)\n",
    "final_iou_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_final_0.csv', index=False)\n",
    "\n",
    "# Print the final IoU values for each ecoregion\n",
    "print(final_iou_df)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = (end_time - start_time) / 60\n",
    "print(f\"Total execution time: {total_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01683951-34a9-4958-9246-10225b9dcf4a",
   "metadata": {},
   "source": [
    "Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadf4cc-76ea-463b-bb6a-7715c6b3a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Setup environment variables\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from MightyMosaic import MightyMosaic\n",
    "import segmentation_models as sm\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "import warnings\n",
    "import glob\n",
    "import tensorflow\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "#image gen class to be used when predicting\n",
    "min_max_vi = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop = True)\n",
    "min_max_vi = min_max_vi[['6', '7', '8']]\n",
    "\n",
    "class img_gen_vi(tensorflow.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\n",
    "    Inputs are batch size, the image size, the input paths (x) and target paths (y)\n",
    "    \"\"\"\n",
    "\n",
    "    #will need pre defined variables batch_size, img_size, input_img_paths and target_img_paths\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "\t    self.batch_size = batch_size\n",
    "\t    self.img_size = img_size\n",
    "\t    self.input_img_paths = input_img_paths\n",
    "\t    self.target_img_paths = input_img_paths\n",
    "\n",
    "    #number of batches the generator is supposed to produceis the length of the paths divided by the batch siize\n",
    "    def __len__(self):\n",
    "\t    return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.input_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (x)\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size] #for a given index get the input batch pathways (y)\n",
    "\t\t\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\") #create matrix of zeros which will have the dimension height, wideth, n_bands), 8 is the n_bands\n",
    "        \n",
    "  \n",
    "         #start populating x by enumerating over the input img paths\n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "\n",
    "           #load image\n",
    "            img =  np.round(np.load(path), 3)\n",
    "            \n",
    "            if img.shape[2] == 4:\n",
    "                \n",
    "                img = img[:, :, :-1]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                img = img[:, :, 6:9]\n",
    "\n",
    "            # img = img * 1000\n",
    "            img = img.astype(float)\n",
    "            img = np.round(img, 3)\n",
    "            img[img == 0] = -999\n",
    "\n",
    "            img[np.isnan(img)] = -999\n",
    "\n",
    "\n",
    "            img[img == -999] = np.nan\n",
    "\n",
    "            in_shape = img.shape\n",
    "            \n",
    "            #turn to dataframe to normalize\n",
    "            img = img.reshape(img.shape[0] * img.shape[1], img.shape[2])\n",
    "\t\t\t\n",
    "            img = pd.DataFrame(img)\n",
    "\t\t\t\n",
    "            img.columns = min_max_vi.columns\n",
    "\t\t\t\n",
    "            img = pd.concat([min_max_vi, img]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "            #normalize 0 to 1\n",
    "            img = pd.DataFrame(scaler.fit_transform(img))\n",
    "\t\t\t\n",
    "            img = img.iloc[2:]\n",
    "#\n",
    "#             img = img.values.reshape(in_shape)\n",
    "            img = img.values.reshape(in_shape)\n",
    "\n",
    "#             replace nan with -1\n",
    "            img[np.isnan(img)] = -1\n",
    "\n",
    "#apply standardization\n",
    "# img = normalize(img, axis=(0,1))\n",
    "\n",
    "            img = np.round(img, 3)\n",
    "            #populate x\n",
    "            x[j] = img#[:, :, 4:] index number is not included, \n",
    "\n",
    "\n",
    "        #do tthe same thing for y\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "\n",
    "            #load image\n",
    "            img =  np.round(np.load(path), 3)[:, :, -1]\n",
    "\n",
    "            img = img.astype(int)\n",
    "\n",
    "            img[img < 0] = 0\n",
    "            img[img >1] = 0\n",
    "            img[~np.isin(img, [0,1])] = 0\n",
    "\n",
    "            img[np.isnan(img)] = 0\n",
    "            img = img.astype(int)\n",
    "\n",
    "            # img =  tf.keras.utils.to_categorical(img, num_classes = 2)\n",
    "            # y[j] = np.expand_dims(img, 2) \n",
    "            y[j] = img\n",
    "  \n",
    "       \n",
    "    #Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "    # y[j] -= 1\n",
    "\n",
    "        return x, y\n",
    "\n",
    "def predict_model(model, generator, name):\n",
    "    \n",
    "    '''\n",
    "    model: tensorflow model to predict\n",
    "    generator: keras generator with the images to predict on\n",
    "    name: string, model name\\\n",
    "    fid: variable I was looping through\n",
    "    count: count retained earlier\n",
    "    '''\n",
    "    #get the results from the nbac and mtbs model\n",
    "    model_1_res = model.evaluate_generator(generator, 100)\n",
    "    # model_1_res = model.evaluate(models_vi_gen, \n",
    "    #                          steps=20,   # Total number of steps (batches)\n",
    "    #                          workers=4,                  # Number of workers for parallel data loading\n",
    "    #                          use_multiprocessing=True)   # Enable multiprocessing for faster data loading\n",
    "\n",
    "    iou = np.round(model_1_res[-2], 2)\n",
    "    precision = np.round(model_1_res[-5], 2)\n",
    "    recall = np.round(model_1_res[-4], 2)\n",
    "    f1 = np.round(model_1_res[-3], 2)\n",
    "    accuracy = np.round(model_1_res[-1], 2)\n",
    "\n",
    "    #make new dataframe with scores\n",
    "    in_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU': [iou],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "                        }, index=[0])  # Explicitly setting index to [0] for a single row\n",
    "\n",
    "    return in_df\n",
    "\n",
    "\n",
    "# Restrict TensorFlow to only use the second GPU (index 1)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set only GPU 1 as visible\n",
    "        tf.config.set_visible_devices(gpus[3], 'GPU')\n",
    "        # Optionally set memory growth if required\n",
    "        tf.config.experimental.set_memory_growth(gpus[3], True)\n",
    "        print(f\"Using GPU: {gpus[3]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# Function to load models for a specific fold\n",
    "def load_models_for_fold(fold):\n",
    "    model_1 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_{fold}_old.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_2 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_3 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_sliding_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_1 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_{fold}_old.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_2 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_ndsi_{fold}.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_3 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_ndsi_sliding_{fold}.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    return model_1, model_2, model_3\n",
    "\n",
    "# Function to filter chunked data for specific ecoregions\n",
    "def filter_chunked(in_names, chunked, data_type):\n",
    "    filtered_chunked = [name for name in chunked if int(name.split('_')[-1].split('.')[0]) in in_names]\n",
    "    base_path = f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_{data_type}_subs_0_128/\"\n",
    "    return [os.path.join(base_path, i) for i in filtered_chunked]\n",
    "\n",
    "# Function to predict using model and accumulate IoU using generator\n",
    "def predict_model_with_generator(model, generator, name):\n",
    "    total_intersection = 0\n",
    "    total_union = 0\n",
    "    \n",
    "    for i in range(len(generator)):\n",
    "        x_batch, y_true = generator[i]\n",
    "        for j in range(len(x_batch)):\n",
    "            x_sample = np.expand_dims(x_batch[j], axis=0)\n",
    "            y_true_sample = y_true[j]\n",
    "\n",
    "            if np.all(y_true_sample == 0):\n",
    "                continue\n",
    "            \n",
    "            y_pred_sample = model.predict(x_sample, verbose=0)\n",
    "            y_pred_sample = np.squeeze(y_pred_sample, axis=1)[0]\n",
    "            y_pred_sample = np.where(y_pred_sample > 0.5, 1, 0)\n",
    "            y_pred_sample = y_pred_sample[:, :, 0]\n",
    "            \n",
    "            intersection = np.logical_and(y_pred_sample, y_true_sample).sum()\n",
    "            union = np.logical_or(y_pred_sample, y_true_sample).sum()\n",
    "            \n",
    "            total_intersection += intersection\n",
    "            total_union += union\n",
    "\n",
    "    iou_calculated = total_intersection / total_union if total_union > 0 else 0\n",
    "    \n",
    "    # Evaluate the model to get metrics including IOU (from model's perspective)\n",
    "    model_1_res = model.evaluate(generator, verbose=0)\n",
    "    \n",
    "    iou_model = np.round(model_1_res[-2], 2)\n",
    "    precision = np.round(model_1_res[-5], 2)\n",
    "    recall = np.round(model_1_res[-4], 2)\n",
    "    f1 = np.round(model_1_res[-3], 2)\n",
    "    accuracy = np.round(model_1_res[-1], 2)\n",
    "    \n",
    "    # Create a dataframe with the results\n",
    "    in_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU (Model)': [iou_model],\n",
    "        'IOU (Calculated)': [iou_calculated],\n",
    "        'Total Intersection': [total_intersection],\n",
    "        'Total Union': [total_union],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "    }, index=[0])\n",
    "    \n",
    "    return in_df, iou_calculated\n",
    "\n",
    "# Main function to calculate IoU for all ecoregions and folds\n",
    "def calculate_iou_across_folds_ecoregion(ecoregions, eco_df, folds, batch_size, img_size):\n",
    "    iou_results = []\n",
    "    \n",
    "    for ecoregion in ecoregions:\n",
    "        print(f\"Processing ecoregion {ecoregion}...\")\n",
    "\n",
    "        total_intersections = {'old': 0, 'ndsi': 0, 'sliding': 0}\n",
    "        total_unions = {'old': 0, 'ndsi': 0, 'sliding': 0}\n",
    "        iou_per_fold = []\n",
    "\n",
    "        sub_eco = eco_df[eco_df['ecoregion'] == ecoregion]\n",
    "        \n",
    "        for fold in folds:\n",
    "            print(f\"Processing fold {fold} for ecoregion {ecoregion}...\")\n",
    "            \n",
    "            # Load models for the fold\n",
    "            model_1, model_2, model_3 = load_models_for_fold(fold)\n",
    "            \n",
    "            # Load testing data for the fold\n",
    "            testing_names = pd.read_csv(f'/explore/nobackup/people/spotter5/cnn_mapping/Russia/test_fold_{fold}.csv')['ID'].tolist()\n",
    "            \n",
    "            # Filter ecoregion and testing names\n",
    "            sub_fold = sub_eco[sub_eco['ID'].isin(testing_names)]\n",
    "            fold_ids = sub_fold['ID'].unique().tolist()\n",
    "            \n",
    "            if len(fold_ids) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Load chunked data\n",
    "            chunked_old = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_old_subs_0_128')\n",
    "            chunked_ndsi = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128')\n",
    "            chunked_sliding = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_sliding_subs_0_128')\n",
    "            \n",
    "            # Filter chunked data by fold IDs and include full paths\n",
    "            testing_names_old = filter_chunked(fold_ids, chunked_old, 'old')\n",
    "            testing_names_ndsi = filter_chunked(fold_ids, chunked_ndsi, 'monthly_ndsi')\n",
    "            testing_names_sliding = filter_chunked(fold_ids, chunked_sliding, 'monthly_ndsi_sliding')\n",
    "\n",
    "            # Generate data for each model\n",
    "            model_vi_gen_old = img_gen_vi(batch_size, img_size, testing_names_old)\n",
    "            model_vi_gen_ndsi = img_gen_vi(batch_size, img_size, testing_names_ndsi)\n",
    "            model_vi_gen_sliding = img_gen_vi(batch_size, img_size, testing_names_sliding)\n",
    "\n",
    "            # Apply the generator and predict for each model\n",
    "            result_old, iou_old = predict_model_with_generator(model_1, model_vi_gen_old, f'Comb_Old_{fold}')\n",
    "            result_ndsi, iou_ndsi = predict_model_with_generator(model_2, model_vi_gen_ndsi, f'Comb_NDSI_{fold}')\n",
    "            result_sliding, iou_sliding = predict_model_with_generator(model_3, model_vi_gen_sliding, f'Comb_Sliding_{fold}')\n",
    "            \n",
    "            # Record IoU for this fold\n",
    "            iou_per_fold.append({\n",
    "                'ecoregion': ecoregion,\n",
    "                'fold': fold,\n",
    "                'iou_old': iou_old,\n",
    "                'iou_ndsi': iou_ndsi,\n",
    "                'iou_sliding': iou_sliding\n",
    "            })\n",
    "\n",
    "            # Accumulate the intersections and unions\n",
    "            total_intersections['old'] += result_old['Total Intersection'].sum()\n",
    "            total_unions['old'] += result_old['Total Union'].sum()\n",
    "            total_intersections['ndsi'] += result_ndsi['Total Intersection'].sum()\n",
    "            total_unions['ndsi'] += result_ndsi['Total Union'].sum()\n",
    "            total_intersections['sliding'] += result_sliding['Total Intersection'].sum()\n",
    "            total_unions['sliding'] += result_sliding['Total Union'].sum()\n",
    "\n",
    "        # Calculate final IoU for this ecoregion across all folds using the sum of intersections and unions\n",
    "        iou_old_final = total_intersections['old'] / total_unions['old'] if total_unions['old'] != 0 else 0\n",
    "        iou_ndsi_final = total_intersections['ndsi'] / total_unions['ndsi'] if total_unions['ndsi'] != 0 else 0\n",
    "        iou_sliding_final = total_intersections['sliding'] / total_unions['sliding'] if total_unions['sliding'] != 0 else 0\n",
    "        \n",
    "        # Store final results\n",
    "        iou_results.append({\n",
    "            'ecoregion': ecoregion,\n",
    "            'final_iou_old': iou_old_final,\n",
    "            'final_iou_ndsi': iou_ndsi_final,\n",
    "            'final_iou_sliding': iou_sliding_final\n",
    "        })\n",
    "\n",
    "    # Return fold-specific and final IoU results\n",
    "    return pd.DataFrame(iou_per_fold), pd.DataFrame(iou_results)\n",
    "\n",
    "# List of folds to process\n",
    "# folds = [0, 2, 4]\n",
    "\n",
    "folds= [0]\n",
    "\n",
    "# Load ecoregion shapefile\n",
    "eco = gpd.read_file('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_good_eco_clip.shp')\n",
    "all_ecoregions = eco['ecoregion'].unique().tolist()\n",
    "\n",
    "# Parameters for image generator\n",
    "batch_size = 20\n",
    "img_size = (128, 128)\n",
    "\n",
    "start_time = time.time()\n",
    "# Calculate IoU across all ecoregions and folds\n",
    "iou_per_fold_df, final_iou_df = calculate_iou_across_folds_ecoregion(all_ecoregions, eco, folds, batch_size, img_size)\n",
    "\n",
    "# Save the results to CSV\n",
    "# iou_per_fold_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_folds.csv', index=False)\n",
    "# final_iou_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_final.csv', index=False)\n",
    "\n",
    "# iou_per_fold_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_folds_0.csv', index=False)\n",
    "# final_iou_df.to_csv('/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/combined_iou_ecoregion_final_0.csv', index=False)\n",
    "\n",
    "# Print the final IoU values for each ecoregion\n",
    "print(final_iou_df)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = (end_time - start_time) / 60\n",
    "print(f\"Total execution time: {total_time:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd2a20-ba56-42ad-9524-332865195fbf",
   "metadata": {},
   "source": [
    "Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460c9ac3-78e2-431a-a5bb-a0c5b3c34696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Processing fold 0...\n",
      "Results saved for fold 0.\n",
      "Processing fold 1...\n",
      "Results saved for fold 1.\n",
      "Processing fold 2...\n",
      "Results saved for fold 2.\n",
      "Processing fold 3...\n",
      "Results saved for fold 3.\n",
      "Processing fold 4...\n",
      "Results saved for fold 4.\n",
      "Total execution time: 161.81 minutes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Setup environment variables\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from MightyMosaic import MightyMosaic\n",
    "import segmentation_models as sm\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "import warnings\n",
    "import glob\n",
    "import tensorflow\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# Image generator class for prediction\n",
    "min_max_vi = pd.read_csv(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_global_min_max_cutoff_proj.csv\").reset_index(drop=True)\n",
    "min_max_vi = min_max_vi[['6', '7', '8']]\n",
    "\n",
    "class img_gen_vi(tensorflow.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = input_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.input_img_paths[i:i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i:i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        \n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "            img = np.round(np.load(path), 3)\n",
    "            if img.shape[2] == 4:\n",
    "                img = img[:, :, :-1]\n",
    "            else:\n",
    "                img = img[:, :, 6:9]\n",
    "            img = img.astype(float)\n",
    "            img[img == 0] = -999\n",
    "            img[np.isnan(img)] = -999\n",
    "            img[img == -999] = np.nan\n",
    "            in_shape = img.shape\n",
    "            img = pd.DataFrame(img.reshape(img.shape[0] * img.shape[1], img.shape[2]))\n",
    "            img.columns = min_max_vi.columns\n",
    "            img = pd.concat([min_max_vi, img]).reset_index(drop=True)\n",
    "            img = pd.DataFrame(scaler.fit_transform(img))\n",
    "            img = img.iloc[2:].values.reshape(in_shape)\n",
    "            img[np.isnan(img)] = -1\n",
    "            x[j] = img\n",
    "\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = np.round(np.load(path), 3)[:, :, -1]\n",
    "            img = img.astype(int)\n",
    "            img[img < 0] = 0\n",
    "            img[img > 1] = 0\n",
    "            img[np.isnan(img)] = 0\n",
    "            y[j] = img\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "\n",
    "def predict_model(model, generator, name):\n",
    "    model_res = model.evaluate(generator, verbose=0)\n",
    "    iou = np.round(model_res[-2], 2)\n",
    "    precision = np.round(model_res[-5], 2)\n",
    "    recall = np.round(model_res[-4], 2)\n",
    "    f1 = np.round(model_res[-3], 2)\n",
    "    accuracy = np.round(model_res[-1], 2)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU': [iou],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "    }, index=[0])\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Restrict TensorFlow to only use the second GPU (index 1)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(f\"Using GPU: {gpus[0]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# Function to load models for a specific fold\n",
    "def load_models_for_fold(fold):\n",
    "    model_1 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_{fold}_old.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_2 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    model_3 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/combined_good_ndsi_sliding_{fold}.tf\", \n",
    "                                         custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                         'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                         'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                         'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_1 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_{fold}_old.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_2 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_ndsi_{fold}.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    # model_3 = tf.keras.models.load_model(f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/models/russia_good_no_regularize_ndsi_sliding_{fold}.tf\", \n",
    "    #                                      custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "    #                                                      'recall': sm.metrics.Recall(threshold=0.5),\n",
    "    #                                                      'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "    #                                                      'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "    return model_1, model_2, model_3\n",
    "\n",
    "# Function to filter chunked data for specific ecoregions\n",
    "def filter_chunked(in_names, chunked, data_type):\n",
    "    filtered_chunked = [name for name in chunked if int(name.split('_')[-1].split('.')[0]) in in_names]\n",
    "    base_path = f\"/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_{data_type}_subs_0_128/\"\n",
    "    return [os.path.join(base_path, i) for i in filtered_chunked]\n",
    "    \n",
    "# Main function to process and save results per fold\n",
    "def process_folds(folds, batch_size, img_size, output_path):\n",
    "    for fold in folds:\n",
    "        print(f\"Processing fold {fold}...\")\n",
    "        \n",
    "        # Load models for the current fold\n",
    "        model_1, model_2, model_3 = load_models_for_fold(fold)\n",
    "        \n",
    "        # Load testing data for the fold\n",
    "        testing_names = pd.read_csv(f'/explore/nobackup/people/spotter5/cnn_mapping/Russia/test_fold_{fold}.csv')['ID'].tolist()\n",
    "\n",
    "        # Load chunked data for old, ndsi, and sliding\n",
    "        chunked_old = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_old_subs_0_128')\n",
    "        chunked_ndsi = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_subs_0_128')\n",
    "        chunked_sliding = os.listdir('/explore/nobackup/people/spotter5/cnn_mapping/Russia/anna_monthly_ndsi_sliding_subs_0_128')\n",
    "\n",
    "        # Filter chunked data based on test names\n",
    "        testing_names_old = filter_chunked(testing_names, chunked_old, 'old')\n",
    "        testing_names_ndsi = filter_chunked(testing_names, chunked_ndsi, 'monthly_ndsi')\n",
    "        testing_names_sliding = filter_chunked(testing_names, chunked_sliding, 'monthly_ndsi_sliding')\n",
    "\n",
    "        # Generate data for each model\n",
    "        model_vi_gen_old = img_gen_vi(batch_size, img_size, testing_names_old)\n",
    "        model_vi_gen_ndsi = img_gen_vi(batch_size, img_size, testing_names_ndsi)\n",
    "        model_vi_gen_sliding = img_gen_vi(batch_size, img_size, testing_names_sliding)\n",
    "\n",
    "        # Apply the generator and predict for each model\n",
    "        result_old = predict_model(model_1, model_vi_gen_old, f'Comb_Old_{fold}')\n",
    "        result_ndsi = predict_model(model_2, model_vi_gen_ndsi, f'Comb_NDSI_{fold}')\n",
    "        result_sliding = predict_model(model_3, model_vi_gen_sliding, f'Comb_Sliding_{fold}')\n",
    "\n",
    "        # # Save results for each fold\n",
    "        # result_old.to_csv(os.path.join(output_path, f'combined_old_{fold}_iou.csv'), index=False)\n",
    "        # result_ndsi.to_csv(os.path.join(output_path, f'combined_ndsi_{fold}_iou.csv'), index=False)\n",
    "        # result_sliding.to_csv(os.path.join(output_path, f'combined_sliding_{fold}_iou.csv'), index=False)\n",
    "\n",
    "         # Concatenate the results for all models into a single dataframe\n",
    "        combined_results = pd.concat([result_old, result_ndsi, result_sliding], ignore_index=True)\n",
    "\n",
    "        # Save the combined results for the fold in one CSV file\n",
    "        combined_results.to_csv(os.path.join(output_path, f'combined_{fold}_iou_t2.csv'), index=False)\n",
    "\n",
    "\n",
    "        print(f\"Results saved for fold {fold}.\")\n",
    "\n",
    "\n",
    "# Parameters and entry point\n",
    "# folds = [0, 1, 2, 4]  # Example fold numbers\n",
    "# folds = [4]\n",
    "\n",
    "folds = range(0, 5)\n",
    "# folds = [4]\n",
    "batch_size = 20  # Example batch size\n",
    "img_size = (128, 128)  # Example image size\n",
    "output_path = '/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each fold and save results\n",
    "process_folds(folds, batch_size, img_size, output_path)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = (end_time - start_time) / 60\n",
    "print(f\"Total execution time: {total_time:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3913f-0b59-4374-b5a4-935cd8963a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning3]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
