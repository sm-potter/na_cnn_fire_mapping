{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5914a4b6-7385-4410-b71f-035f283f0a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Setup environment variables\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from MightyMosaic import MightyMosaic\n",
    "import segmentation_models as sm\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "import warnings\n",
    "import glob\n",
    "import tensorflow\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e513a416-467e-48b0-8661-0799abf69683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class img_gen_vi(tensorflow.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\n",
    "    Inputs are batch size, the image size, the input paths (x) and target paths (y)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = input_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) corresponding to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "\n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "            try:\n",
    "                # Attempt to load the image\n",
    "                img = np.round(np.load(path, allow_pickle=True), 3)\n",
    "\n",
    "                if img.shape[2] == 4:\n",
    "                    img = img[:, :, :-1]\n",
    "                else:\n",
    "                    img = img[:, :, 6:9]\n",
    "\n",
    "                img = img.astype(float)\n",
    "                img = np.round(img, 3)\n",
    "                img[img == 0] = -999\n",
    "                img[np.isnan(img)] = -999\n",
    "                img[img == -999] = np.nan\n",
    "\n",
    "                in_shape = img.shape\n",
    "\n",
    "                # Turn to dataframe to normalize\n",
    "                img = img.reshape(img.shape[0] * img.shape[1], img.shape[2])\n",
    "                img = pd.DataFrame(img)\n",
    "                img.columns = min_max_vi.columns\n",
    "\n",
    "                # Add min/max for normalization\n",
    "                img = pd.concat([min_max_vi, img]).reset_index(drop=True)\n",
    "\n",
    "                # Normalize 0 to 1\n",
    "                img = pd.DataFrame(scaler.fit_transform(img))\n",
    "                img = img.iloc[2:]  # Remove the added rows for min/max\n",
    "\n",
    "                img = img.values.reshape(in_shape)\n",
    "                img[np.isnan(img)] = -1\n",
    "\n",
    "                img = np.round(img, 3)\n",
    "\n",
    "                x[j] = img  # Populate x\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {path}: {e}\")\n",
    "                continue  # Skip this file and continue with the next one\n",
    "\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            try:\n",
    "                img = np.round(np.load(path, allow_pickle=True), 3)[:, :, -1]\n",
    "                img = img.astype(int)\n",
    "                img[img < 0] = 0\n",
    "                img[img > 1] = 0\n",
    "                img[~np.isin(img, [0, 1])] = 0\n",
    "                img[np.isnan(img)] = 0\n",
    "                img = img.astype(int)\n",
    "\n",
    "                y[j] = img\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {path}: {e}\")\n",
    "                continue  # Skip this file and continue with the next one\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b272403c-a3ab-48e0-8c8e-ae924a02d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming min_max_vi and scaler are already defined as in your original code\n",
    "\n",
    "class img_gen_vi_one(tf.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\n",
    "    Inputs are batch size (now fixed to 1), the image size, the input paths (x), and target paths (y)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, input_img_paths):\n",
    "        self.batch_size = 1  # Fixed batch size to 1 for individual processing\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = input_img_paths  # Assuming target paths are the same\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_img_paths)  # One batch per image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) corresponding to batch #idx.\"\"\"\n",
    "        \n",
    "        # Get the image path\n",
    "        img_path = self.input_img_paths[idx]\n",
    "        \n",
    "        # Create empty arrays for x (input) and y (ground truth)\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        y = np.zeros((self.batch_size,) + self.img_size, dtype=\"uint8\")\n",
    "        \n",
    "        # Load image\n",
    "        img = np.round(np.load(img_path), 3)\n",
    "        \n",
    "        # Select the appropriate bands for normalization\n",
    "        if img.shape[2] == 4:\n",
    "            img = img[:, :, :-1]  # Drop the last band if it has 4 bands\n",
    "        else:\n",
    "            img = img[:, :, 6:9]  # Select bands 6 to 8 if it has more than 3 bands\n",
    "\n",
    "        # Normalize the image\n",
    "        img = img.astype(float)\n",
    "        img = np.round(img, 3)\n",
    "        img[img == 0] = -999\n",
    "        img[np.isnan(img)] = -999\n",
    "        img[img == -999] = np.nan\n",
    "\n",
    "        in_shape = img.shape\n",
    "        img = img.reshape(img.shape[0] * img.shape[1], img.shape[2])\n",
    "        img = pd.DataFrame(img, columns=min_max_vi.columns)\n",
    "        img = pd.concat([min_max_vi, img]).reset_index(drop=True)\n",
    "        img = pd.DataFrame(scaler.transform(img))\n",
    "        img = img.iloc[len(min_max_vi):]  # Remove the first rows from the min_max scaling\n",
    "        img = img.values.reshape(in_shape)\n",
    "        img[np.isnan(img)] = -1\n",
    "\n",
    "        # Assign the normalized image to the input array\n",
    "        x[0] = img\n",
    "\n",
    "        # Ground truth (y) is the last band\n",
    "        y_img = np.round(np.load(img_path), 3)[:, :, -1]\n",
    "        y_img = y_img.astype(int)\n",
    "        y_img[y_img < 0] = 0\n",
    "        y_img[y_img > 1] = 0\n",
    "        y_img[~np.isin(y_img, [0, 1])] = 0\n",
    "        y_img[np.isnan(y_img)] = 0\n",
    "        y[0] = y_img\n",
    "\n",
    "        return x, y\n",
    "        \n",
    "# Initialize the generator with batch size 1\n",
    "# models_vi_gen = img_gen_vi_one(img_size, chunk_files)\n",
    "\n",
    "# # Predict the model using the generator with batch size 1\n",
    "# gen_preds = predict_model(model, models_vi_gen, 'Comb_Sliding_1')\n",
    "\n",
    "# print(gen_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dfd5091-ea4d-4709-8d68-c0ca9e397d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading /explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_old_subs_0_128/5_1_5976.npy: Failed to interpret file '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_old_subs_0_128/5_1_5976.npy' as a pickle\n",
      "Error loading /explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_old_subs_0_128/5_1_5976.npy: Failed to interpret file '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_old_subs_0_128/5_1_5976.npy' as a pickle\n",
      "Error loading /explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_old_subs_0_128/5_1_5976.npy: Failed to interpret file '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_old_subs_0_128/5_1_5976.npy' as a pickle\n",
      "Error loading /explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_old_subs_0_128/5_1_5976.npy: Failed to interpret file '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_old_subs_0_128/5_1_5976.npy' as a pickle\n",
      "Results saved to /explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp/north_america.csv\n",
      "Total execution time: 98.78 minutes\n",
      "Overall IoU for old model across all folds: 0.879487944335838\n",
      "Overall IoU for NDSI model across all folds: 0.8655942266804221\n",
      "Overall IoU for Sliding NDSI model across all folds: 0.8768355529974867\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to load models for a specific fold\n",
    "\n",
    "model_1 = tf.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/models/nbac_mtbs_regularize_50_global_norm_old.tf\", \n",
    "                                     custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                     'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                     'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                     'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "model_2 = tf.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/models/nbac_mtbs_regularize_50_global_norm_ndsi.tf\", \n",
    "                                     custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                     'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                     'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                     'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "model_3 = tf.keras.models.load_model(\"/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/models/nbac_mtbs_ndsi_sliding.tf\", \n",
    "                                     custom_objects={'precision': sm.metrics.Precision(threshold=0.5), \n",
    "                                                     'recall': sm.metrics.Recall(threshold=0.5),\n",
    "                                                     'f1-score': sm.metrics.FScore(threshold=0.5),\n",
    "                                                     'iou_score': sm.metrics.IOUScore(threshold=0.5)})\n",
    "\n",
    "\n",
    "\n",
    "# Function to predict using model and accumulate IoU across batches\n",
    "def predict_model(model, generator, name):\n",
    "    total_intersection = 0\n",
    "    total_union = 0\n",
    "    \n",
    "    for i in range(len(generator)):\n",
    "        x_batch, y_true = generator[i]\n",
    "        for j in range(len(x_batch)):\n",
    "            x_sample = np.expand_dims(x_batch[j], axis=0)\n",
    "            y_true_sample = y_true[j]\n",
    "\n",
    "            if np.all(y_true_sample == 0):\n",
    "                continue\n",
    "            \n",
    "            y_pred_sample = model.predict(x_sample, verbose=0)\n",
    "            y_pred_sample = np.squeeze(y_pred_sample, axis=1)[0]\n",
    "            y_pred_sample = np.where(y_pred_sample > 0.5, 1, 0)\n",
    "            y_pred_sample = y_pred_sample[:, :, 0]\n",
    "            \n",
    "            assert y_pred_sample.shape == y_true_sample.shape, f\"Shape mismatch: y_pred {y_pred_sample.shape} and y_true {y_true_sample.shape}\"\n",
    "            \n",
    "            intersection = np.logical_and(y_pred_sample, y_true_sample).sum()\n",
    "            union = np.logical_or(y_pred_sample, y_true_sample).sum()\n",
    "            \n",
    "            total_intersection += intersection\n",
    "            total_union += union\n",
    "    \n",
    "    iou_calculated = total_intersection / total_union if total_union > 0 else 0\n",
    "    \n",
    "    # Evaluate the model to get metrics including IOU (from model's perspective)\n",
    "    model_1_res = model.evaluate(generator, verbose=0)\n",
    "    \n",
    "    iou_model = np.round(model_1_res[-2], 2)\n",
    "    precision = np.round(model_1_res[-5], 2)\n",
    "    recall = np.round(model_1_res[-4], 2)\n",
    "    f1 = np.round(model_1_res[-3], 2)\n",
    "    accuracy = np.round(model_1_res[-1], 2)\n",
    "    \n",
    "    # Create a dataframe with the results\n",
    "    in_df = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'IOU (Model)': [iou_model],\n",
    "        'IOU (Calculated)': [iou_calculated],\n",
    "        'Total Intersection': [total_intersection],\n",
    "        'Total Union': [total_union],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F-1': [f1],\n",
    "        'Accuracy': [accuracy]\n",
    "    }, index=[0])\n",
    "    \n",
    "    return in_df\n",
    "\n",
    "# Function to process all folds dynamically for each model\n",
    "def process_all_folds(batch_size, img_size, output_path):\n",
    "    \n",
    "    total_intersections = {'old': 0, 'ndsi': 0, 'sliding': 0}\n",
    "    total_unions = {'old': 0, 'ndsi': 0, 'sliding': 0}\n",
    "    results = []\n",
    "\n",
    "        \n",
    "    testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_old_testing_files.csv')['Files'].tolist()\n",
    "    testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_old_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "    testing_names_old = testing_names + testing_names2\n",
    "\n",
    "    testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_monthly_ndsi_testing_files.csv')['Files'].tolist()\n",
    "    testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_monthly_ndsi_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "    testing_names_ndsi = testing_names + testing_names2\n",
    "\n",
    "    testing_names = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_monthly_ndsi_sliding_testing_files.csv')['Files'].tolist()\n",
    "    testing_names2 = pd.read_csv('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/mtbs_monthly_ndsi_sliding_testing_files.csv')['Files'].tolist()\n",
    "\n",
    "    testing_names_sliding = testing_names + testing_names2\n",
    "    \n",
    "    # Generate data for each model\n",
    "    model_vi_gen_old = img_gen_vi(batch_size, img_size, testing_names_old)\n",
    "    model_vi_gen_ndsi = img_gen_vi(batch_size, img_size, testing_names_ndsi)\n",
    "    model_vi_gen_sliding = img_gen_vi(batch_size, img_size, testing_names_sliding)\n",
    "\n",
    "    # Apply the generator and predict for each model\n",
    "    result_old = predict_model(model_1, model_vi_gen_old, 'Comb_Old')\n",
    "    result_ndsi = predict_model(model_2, model_vi_gen_ndsi, 'Comb_NDSI')\n",
    "    result_sliding = predict_model(model_3, model_vi_gen_sliding, 'Comb_Sliding')\n",
    "\n",
    "    results.append(result_old)\n",
    "    results.append(result_ndsi)\n",
    "    results.append(result_sliding)\n",
    "\n",
    "    # Accumulate the intersections and unions\n",
    "    total_intersections['old'] += result_old['Total Intersection'].sum()\n",
    "    total_unions['old'] += result_old['Total Union'].sum()\n",
    "    total_intersections['ndsi'] += result_ndsi['Total Intersection'].sum()\n",
    "    total_unions['ndsi'] += result_ndsi['Total Union'].sum()\n",
    "    total_intersections['sliding'] += result_sliding['Total Intersection'].sum()\n",
    "    total_unions['sliding'] += result_sliding['Total Union'].sum()\n",
    "\n",
    "    # Calculate the final IoU for each model\n",
    "    iou_old_final = total_intersections['old'] / total_unions['old'] if total_unions['old'] != 0 else 0\n",
    "    iou_ndsi_final = total_intersections['ndsi'] / total_unions['ndsi'] if total_unions['ndsi'] != 0 else 0\n",
    "    iou_sliding_final = total_intersections['sliding'] / total_unions['sliding'] if total_unions['sliding'] != 0 else 0\n",
    "\n",
    "    # Create a final results dataframe\n",
    "    final_results = pd.DataFrame({\n",
    "        'Model': ['Overall_Old', 'Overall_NDSI', 'Overall_Sliding'],\n",
    "        'IOU (Calculated)': [iou_old_final, iou_ndsi_final, iou_sliding_final],\n",
    "        'Total Intersection': [total_intersections['old'], total_intersections['ndsi'], total_intersections['sliding']],\n",
    "        'Total Union': [total_unions['old'], total_unions['ndsi'], total_unions['sliding']]\n",
    "    })\n",
    "\n",
    "    # Concatenate fold results with overall results\n",
    "    all_results = pd.concat([pd.concat(results, ignore_index=True), final_results], ignore_index=True)\n",
    "\n",
    "    # Save results to CSV\n",
    "    output_file = os.path.join(output_path, 'north_america.csv')\n",
    "    all_results.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "    # Return the final results\n",
    "    return iou_old_final, iou_ndsi_final, iou_sliding_final\n",
    "\n",
    "# Main entry point\n",
    "batch_size = 20  # Example batch size\n",
    "img_size = (128, 128)  # Example image size\n",
    "output_path = '/explore/nobackup/people/spotter5/cnn_mapping/Russia/spatial_compare_temp'\n",
    "os.makedirs(output_path, exist_ok = True)\n",
    "\n",
    "# Process all folds and get the final IoU for each model\n",
    "start_time = time.time()\n",
    "iou_old, iou_ndsi, iou_sliding = process_all_folds(batch_size, img_size, output_path)\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = (end_time - start_time) / 60\n",
    "print(f\"Total execution time: {total_time:.2f} minutes\")\n",
    "\n",
    "# Print the final IoU for each model across all folds\n",
    "print(f\"Overall IoU for old model across all folds: {iou_old}\")\n",
    "print(f\"Overall IoU for NDSI model across all folds: {iou_ndsi}\")\n",
    "print(f\"Overall IoU for Sliding NDSI model across all folds: {iou_sliding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16f7b072-fc7e-4841-96c3-e881a604bca4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_old_subs_0_128/5_1_5976.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m l\n",
      "File \u001b[0;32m~/.conda/envs/deeplearning3/lib/python3.10/site-packages/numpy/lib/npyio.py:438\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[0;32m--> 438\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load file containing pickled data \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen allow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(fid, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "l = np.load('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/nbac_old_subs_0_128/5_1_5976.npy')\n",
    "\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b84f0fe-aa88-4901-9be0-59e736c7afa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning3]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
