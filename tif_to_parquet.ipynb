{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5252af2b-c611-4cc0-b1ae-e67a626a4f69",
   "metadata": {},
   "source": [
    "This notebook will take the tif files downloaded in 'get_training.ipynb' and turn it to a csv, which is then used to train the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8902bf-8542-4f48-8a6f-c17cb36901b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf16acfd-4bac-4b58-a154-3d28a6903cd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'in_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#start with mtbs\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m all_files_mtbs:\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#read in file and convert to numpy\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     in_file \u001b[38;5;241m=\u001b[39m rioxarray\u001b[38;5;241m.\u001b[39mopen_rasterio(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43min_path\u001b[49m, f))\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m in_file[:, :, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'in_path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "#don't use files if all 0's\n",
    "\n",
    "#check if all 0\n",
    "def is_matrix_all_zeros(matrix):\n",
    "    # Convert the matrix to a NumPy array\n",
    "    np_matrix = np.array(matrix)\n",
    "\n",
    "    # Check if all elements in the array are zeros\n",
    "    return np.all(np_matrix == 0)\n",
    "\n",
    "#outpath\n",
    "out = '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files'\n",
    "os.makedirs(out, exist_ok = True)\n",
    "\n",
    "\n",
    "#code to read in all training data\n",
    "all_files_mtbs =  glob.glob('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj_mtbs/*.tif')\n",
    "\n",
    "all_files_nbac =  glob.glob('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj/*.tif')\n",
    "\n",
    "#empty list for combining all data\n",
    "combined_training = []\n",
    "\n",
    "#start with mtbs\n",
    "for f in all_files_mtbs:\n",
    "\n",
    "    #read in file and convert to numpy\n",
    "    in_file = rioxarray.open_rasterio(os.path.join('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj_mtbs', f)).to_numpy()\n",
    "    \n",
    "    #convert to band last\n",
    "    in_file= np.moveaxis(in_file, 0, 2) \n",
    "\n",
    "    x = in_file[:, :, :-1]\n",
    "    x = x.astype(float)\n",
    "    x[x == 0] = np.nan\n",
    "    \n",
    "    x = np.round(x, 2)\n",
    "    \n",
    "    y = in_file[:, :, -1]\n",
    "    y = y.astype(float)\n",
    "    y[y <0 ] = 0\n",
    "    y[y >1 ] = 0\n",
    "       \n",
    "    y[~np.isin(y, [0,1])] = np.nan\n",
    "    \n",
    "    #reshape the 3D matrix to 2D\n",
    "    x, y, z = in_file.shape  # Get the 'x' dimension\n",
    "    # matrix_2d = matrix_3d.reshape(x*x, 10)\n",
    "\n",
    "     #convert to pandas dataframe\n",
    "    reshaped_data = stacked.reshape(x*y, z)\n",
    "\n",
    "    band_names = ['blue', 'green', 'red', 'NIR', 'SWIR1', 'SWIR2', 'dNBR', 'dNDVI', 'dNDII', 'y']\n",
    "\n",
    "    # Create a DataFrame\n",
    "    training = pd.DataFrame(reshaped_data, columns=band_names)\n",
    "\n",
    "#     #original values were originally scaled\n",
    "#     columns_to_divide = [col for col in training.columns if col != 'y']\n",
    "\n",
    "#     # Divide selected columns by 1000\n",
    "#     training[columns_to_divide] = training[columns_to_divide].div(1000).round(3)\n",
    "    \n",
    "#     # training['Fname'] = f.replace('.tif', '')\n",
    "    \n",
    "#     # training = training[['Fname', 'dNBR', 'dNDVI', 'dNDII', 'y']]\n",
    "#     training= training[~(training == 0).all(axis=1)]\n",
    "#     training = training[training['y'].isin([0, 1])]\n",
    "    \n",
    "    #append to list\n",
    "    combined_training.append(training)\n",
    "    \n",
    "  \n",
    "#concat\n",
    "combined_training = pd.concat(combined_training, ignore_index=True)#.dropna()\n",
    "\n",
    "# combined_training.head()\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time in seconds\n",
    "elapsed_time_seconds = end_time - start_time\n",
    "\n",
    "\n",
    "combined_training.to_parquet(os.path.join(out, 'all_training_mtbs.parquet'), index = False, engine = 'pyarrow')\n",
    "\n",
    "# Convert seconds to minutes\n",
    "elapsed_time_minutes = elapsed_time_seconds / 60\n",
    "\n",
    "print(f\"MTBS Elapsed time: {elapsed_time_minutes:.2f} minutes\")\n",
    "    \n",
    "#now do nbac\n",
    "#empty list for combining all data\n",
    "combined_training = []\n",
    "\n",
    "#start with mtbs\n",
    "for f in all_files_mtbs:\n",
    "\n",
    "    #read in file and convert to numpy\n",
    "    in_file = rioxarray.open_rasterio(os.path.join('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj_nbac', f)).to_numpy()\n",
    "    \n",
    "    #convert to band last\n",
    "    in_file= np.moveaxis(in_file, 0, 2) \n",
    "\n",
    "    x = in_file[:, :, :-1]\n",
    "    x = x.astype(float)\n",
    "    x[x == 0] = np.nan\n",
    "    \n",
    "    x = np.round(x, 2)\n",
    "    \n",
    "    y = in_file[:, :, -1]\n",
    "    y = y.astype(float)\n",
    "    y[y <0 ] = 0\n",
    "    y[y >1 ] = 0\n",
    "       \n",
    "    y[~np.isin(y, [0,1])] = np.nan\n",
    "    \n",
    "    #reshape the 3D matrix to 2D\n",
    "    x, y, z = in_file.shape  # Get the 'x' dimension\n",
    "    # matrix_2d = matrix_3d.reshape(x*x, 10)\n",
    "\n",
    "     #convert to pandas dataframe\n",
    "    reshaped_data = stacked.reshape(x*y, z)\n",
    "\n",
    "    band_names = ['blue', 'green', 'red', 'NIR', 'SWIR1', 'SWIR2', 'dNBR', 'dNDVI', 'dNDII', 'y']\n",
    "\n",
    "    # Create a DataFrame\n",
    "    training = pd.DataFrame(reshaped_data, columns=band_names)\n",
    "\n",
    "#     #original values were originally scaled\n",
    "#     columns_to_divide = [col for col in training.columns if col != 'y']\n",
    "\n",
    "#     # Divide selected columns by 1000\n",
    "#     training[columns_to_divide] = training[columns_to_divide].div(1000).round(3)\n",
    "    \n",
    "#     # training['Fname'] = f.replace('.tif', '')\n",
    "    \n",
    "#     # training = training[['Fname', 'dNBR', 'dNDVI', 'dNDII', 'y']]\n",
    "#     training= training[~(training == 0).all(axis=1)]\n",
    "#     training = training[training['y'].isin([0, 1])]\n",
    "    \n",
    "    #append to list\n",
    "    combined_training.append(training)\n",
    "    \n",
    "  \n",
    "#concat\n",
    "combined_training = pd.concat(combined_training, ignore_index=True)#.dropna()\n",
    "\n",
    "# combined_training.head()\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time in seconds\n",
    "elapsed_time_seconds = end_time - start_time\n",
    "\n",
    "\n",
    "combined_training.to_parquet(os.path.join(out, 'all_training_nbac.parquet'), index = False, engine = 'pyarrow')\n",
    "\n",
    "# Convert seconds to minutes\n",
    "elapsed_time_minutes = elapsed_time_seconds / 60\n",
    "\n",
    "print(f\"MTBS Elapsed time: {elapsed_time_minutes:.2f} minutes\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0abb4a-7059-482c-ad6f-1ebc05987725",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_nbac.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mtbs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_mtbs.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m nbac \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_nbac.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(mtbs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(nbac\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.conda/envs/deeplearning3/lib/python3.10/site-packages/pandas/io/parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/deeplearning3/lib/python3.10/site-packages/pandas/io/parquet.py:220\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    218\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilesystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    228\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    229\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/deeplearning3/lib/python3.10/site-packages/pandas/io/parquet.py:110\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/deeplearning3/lib/python3.10/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_nbac.parquet'"
     ]
    }
   ],
   "source": [
    "mtbs = pd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_mtbs.parquet')\n",
    "nbac = pd.read_parquet('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files/all_training_nbac.parquet')\n",
    "print(mtbs.shape)\n",
    "print(nbac.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e395cabe-db26-41eb-b756-723c07c71eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468203344, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtbs.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c3807ea-1f21-4934-8da8-d726c5c94f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>NIR</th>\n",
       "      <th>SWIR1</th>\n",
       "      <th>SWIR2</th>\n",
       "      <th>dNBR</th>\n",
       "      <th>dNDVI</th>\n",
       "      <th>dNDII</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-101.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>-104.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-91.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-107.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-10.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-33.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-74.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>-33.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-101.0</td>\n",
       "      <td>-144.0</td>\n",
       "      <td>-94.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blue  green   red   NIR  SWIR1  SWIR2   dNBR  dNDVI  dNDII    y\n",
       "0  -5.0   -3.0   3.0 -32.0   20.0   14.0 -101.0  -45.0 -104.0  0.0\n",
       "1  -5.0    1.0   6.0 -28.0   24.0   12.0  -91.0  -54.0 -107.0  0.0\n",
       "2 -10.0  -10.0  -5.0 -33.0    6.0    4.0  -66.0   -5.0  -74.0  0.0\n",
       "3   9.0   11.0  16.0 -26.0   -6.0    2.0  -49.0 -105.0  -33.0  0.0\n",
       "4   8.0    9.0  22.0 -17.0   22.0   17.0 -101.0 -144.0  -94.0  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1d8162-e233-4a8c-bf4e-8ad187ed996b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 182\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m#concat\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m combined_training \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.dropna()\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# combined_training.head()\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Record the end time\u001b[39;00m\n\u001b[1;32m    187\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/.conda/envs/deeplearning3/lib/python3.10/site-packages/pandas/core/reshape/concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/.conda/envs/deeplearning3/lib/python3.10/site-packages/pandas/core/reshape/concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    426\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import rioxarray \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "#don't use files if all 0's\n",
    "\n",
    "#check if all 0\n",
    "def is_matrix_all_zeros(matrix):\n",
    "    # Convert the matrix to a NumPy array\n",
    "    np_matrix = np.array(matrix)\n",
    "\n",
    "    # Check if all elements in the array are zeros\n",
    "    return np.all(np_matrix == 0)\n",
    "\n",
    "#outpath\n",
    "out = '/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/parquet_files'\n",
    "os.makedirs(out, exist_ok = True)\n",
    "\n",
    "\n",
    "#code to read in all training data\n",
    "all_files_mtbs =  glob.glob('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj_mtbs/*.tif')\n",
    "\n",
    "all_files_nbac =  glob.glob('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj/*.tif')\n",
    "\n",
    "#too many files for nbac, need to sample\\\n",
    "#50% sample size\n",
    "# sample_size = len(all_files_nbac) // 2\n",
    "\n",
    "# all_files_nbac = random.sample(all_files_nbac, sample_size)\n",
    "\n",
    "# #empty list for combining all data\n",
    "# combined_training = []\n",
    "\n",
    "# #start with mtbs\n",
    "# for f in all_files_mtbs:\n",
    "    \n",
    "#     print(f)\n",
    "\n",
    "#     #read in file and convert to numpy\n",
    "#     in_file = rioxarray.open_rasterio(os.path.join('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj_mtbs', f)).to_numpy()\n",
    "    \n",
    "#     #convert to band last\n",
    "#     in_file= np.moveaxis(in_file, 0, 2) \n",
    "\n",
    "#     x = in_file[:, :, :-1]\n",
    "#     x = x.astype(float)\n",
    "#     x[x == 0] = np.nan\n",
    "    \n",
    "#     x = np.round(x, 2)\n",
    "    \n",
    "#     y = in_file[:, :, -1]\n",
    "#     y = y.astype(float)\n",
    "#     y[y <0 ] = 0\n",
    "#     y[y >1 ] = 0\n",
    "    \n",
    "#     y[~np.isin(y, [0,1])] = np.nan\n",
    "    \n",
    "#     y = np.round(y, 2)\n",
    "    \n",
    "#     stacked = np.dstack([x, y])\n",
    "    \n",
    "#     #reshape the 3D matrix to 2D\n",
    "#     x, y, z = in_file.shape  \n",
    "    \n",
    "#      #convert to pandas dataframe\n",
    "#     reshaped_data = stacked.reshape(x*y, z)\n",
    "\n",
    "#     band_names = ['blue', 'green', 'red', 'NIR', 'SWIR1', 'SWIR2', 'dNBR', 'dNDVI', 'dNDII', 'y']\n",
    "\n",
    "#     # Create a DataFrame\n",
    "#     training = pd.DataFrame(reshaped_data, columns=band_names)\n",
    "\n",
    "# #     #original values were originally scaled\n",
    "# #     columns_to_divide = [col for col in training.columns if col != 'y']\n",
    "\n",
    "# #     # Divide selected columns by 1000\n",
    "# #     training[columns_to_divide] = training[columns_to_divide].div(1000).round(3)\n",
    "    \n",
    "# #     # training['Fname'] = f.replace('.tif', '')\n",
    "    \n",
    "# #     # training = training[['Fname', 'dNBR', 'dNDVI', 'dNDII', 'y']]\n",
    "# #     training= training[~(training == 0).all(axis=1)]\n",
    "# #     training = training[training['y'].isin([0, 1])]\n",
    "    \n",
    "#     #append to list\n",
    "#     combined_training.append(training)\n",
    "    \n",
    "  \n",
    "# #concat\n",
    "# combined_training = pd.concat(combined_training, ignore_index=True)#.dropna()\n",
    "\n",
    "# # combined_training.head()\n",
    "\n",
    "# # Record the end time\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate the elapsed time in seconds\n",
    "# elapsed_time_seconds = end_time - start_time\n",
    "\n",
    "\n",
    "# combined_training.to_parquet(os.path.join(out, 'all_training_mtbs.parquet'), index = False)\n",
    "\n",
    "# Convert seconds to minutes\n",
    "# elapsed_time_minutes = elapsed_time_seconds / 60\n",
    "\n",
    "# print(f\"MTBS Elapsed time: {elapsed_time_minutes:.2f} minutes\")\n",
    "    \n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "#now do nbac\n",
    "#empty list for combining all data\n",
    "combined_training = []\n",
    "\n",
    "#start with mtbs\n",
    "for f in all_files_nbac:\n",
    "    \n",
    "    # print(f)\n",
    "    try:\n",
    "        #read in file and convert to numpy\n",
    "        # in_file = rioxarray.open_rasterio(os.path.join('/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj', f)).to_numpy().astype(np.int16)\n",
    "        in_file = rioxarray.open_rasterio(os.path.join(f)).to_numpy().astype(float)\n",
    "\n",
    "        #convert to band last\n",
    "        in_file= np.moveaxis(in_file, 0, 2) \n",
    "\n",
    "        x = in_file[:, :, :-1]\n",
    "        # x = x.astype(float)\n",
    "        x[x == 0] = np.nan\n",
    "\n",
    "        x = np.round(x, 2)\n",
    "\n",
    "        y = in_file[:, :, -1]\n",
    "        y = y.astype(float)\n",
    "        y[y <0 ] = 0\n",
    "        y[y >1 ] = 0\n",
    "\n",
    "        # y[~np.isin(y, [0,1])] = np.nan\n",
    "\n",
    "#         y = np.round(y, 2)\n",
    "        stacked = np.dstack([x, y])\n",
    "\n",
    "        #reshape the 3D matrix to 2D\n",
    "        x, y, z = in_file.shape  # Get the 'x' dimension\n",
    "        # matrix_2d = matrix_3d.reshape(x*x, 10)\n",
    "\n",
    "         #convert to pandas dataframe\n",
    "        reshaped_data = stacked.reshape(x*y, z)\n",
    "\n",
    "        band_names = ['blue', 'green', 'red', 'NIR', 'SWIR1', 'SWIR2', 'dNBR', 'dNDVI', 'dNDII', 'y']\n",
    "\n",
    "        # Create a DataFrame\n",
    "        training = pd.DataFrame(reshaped_data, columns=band_names)\n",
    "\n",
    "    #     #original values were originally scaled\n",
    "    #     columns_to_divide = [col for col in training.columns if col != 'y']\n",
    "\n",
    "    #     # Divide selected columns by 1000\n",
    "    #     training[columns_to_divide] = training[columns_to_divide].div(1000).round(3)\n",
    "\n",
    "    #     # training['Fname'] = f.replace('.tif', '')\n",
    "\n",
    "    #     # training = training[['Fname', 'dNBR', 'dNDVI', 'dNDII', 'y']]\n",
    "    #     training= training[~(training == 0).all(axis=1)]\n",
    "    #     training = training[training['y'].isin([0, 1])]\n",
    "\n",
    "        #append to list\n",
    "        combined_training.append(training)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "  \n",
    "#concat\n",
    "combined_training = pd.concat(combined_training, ignore_index=True)#.dropna()\n",
    "\n",
    "# combined_training.head()\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time in seconds\n",
    "elapsed_time_seconds = end_time - start_time\n",
    "\n",
    "\n",
    "combined_training.to_parquet(os.path.join(out, 'all_training_nbac.parquet'), index = False)\n",
    "\n",
    "# Convert seconds to minutes\n",
    "elapsed_time_minutes = elapsed_time_seconds / 60\n",
    "\n",
    "print(f\"MTBS Elapsed time: {elapsed_time_minutes:.2f} minutes\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b3e82-118a-4d42-8c4c-9291717b11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5be3bcb-e71b-4c0b-9a3b-8fb757ec36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    " #read in file and convert to numpy\n",
    "in_file = rioxarray.open_rasterio(f).to_numpy().astype(float)\n",
    "\n",
    "#convert to band last\n",
    "in_file= np.moveaxis(in_file, 0, 2) \n",
    "\n",
    "x = in_file[:, :, :-1]\n",
    "# x = x.astype(float)\n",
    "x[x == 0] = np.nan\n",
    "\n",
    "x = np.round(x, 2)\n",
    "\n",
    "y = in_file[:, :, -1]\n",
    "y = y.astype(float)\n",
    "y[y <0 ] = 0\n",
    "y[y >1 ] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4816b17c-ddba-4c01-849f-3217c3657ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/explore/nobackup/people/spotter5/cnn_mapping/nbac_training/l8_sent_collection2_proj/median_9947.tif'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855ed98-3489-49cd-b54a-f33fdb5389e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deeplearning3]",
   "language": "python",
   "name": "conda-env-.conda-deeplearning3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
